import{_ as a,c as n,b as o,o as e}from"./chunks/framework.B1z0IdBH.js";const u=JSON.parse('{"title":"AWQ","description":"","frontmatter":{},"headers":[{"level":2,"title":"Usage of AWQ Models with Hugging Face transformers","slug":"usage-of-awq-models-with-hugging-face-transformers","link":"#usage-of-awq-models-with-hugging-face-transformers","children":[]},{"level":2,"title":"Usage of AWQ  Models with vLLM","slug":"usage-of-awq-models-with-vllm","link":"#usage-of-awq-models-with-vllm","children":[]},{"level":2,"title":"Quantize Your Own Model with AutoAWQ","slug":"quantize-your-own-model-with-autoawq","link":"#quantize-your-own-model-with-autoawq","children":[]}],"relativePath":"guide/quantization/awq.md","filePath":"guide/quantization/awq.md"}'),l={name:"guide/quantization/awq.md"};function p(t,s,r,c,E,i){return e(),n("div",null,s[0]||(s[0]=[o(`<h1 id="awq" tabindex="-1">AWQ <a class="header-anchor" href="#awq" aria-label="Permalink to &quot;AWQ&quot;">​</a></h1><p>For quantized models, one of our recommendations is the usage of <a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noreferrer">AWQ</a> with <a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noreferrer">AutoAWQ</a>.</p><p><strong>AWQ</strong> refers to Activation-aware Weight Quantization, a hardware-friendly approach for LLM low-bit weight-only quantization.</p><p><strong>AutoAWQ</strong> is an easy-to-use Python library for 4-bit quantized models. AutoAWQ speeds up models by 3x and reduces memory requirements by 3x compared to FP16. AutoAWQ implements the Activation-aware Weight Quantization (AWQ) algorithm for quantizing LLMs.</p><p>In this document, we show you how to use the quantized model with Hugging Face <code>transformers</code> and also how to quantize your own model.</p><h2 id="usage-of-awq-models-with-hugging-face-transformers" tabindex="-1">Usage of AWQ Models with Hugging Face transformers <a class="header-anchor" href="#usage-of-awq-models-with-hugging-face-transformers" aria-label="Permalink to &quot;Usage of AWQ Models with Hugging Face transformers&quot;">​</a></h2><p>Now, <code>transformers</code> has officially supported AutoAWQ, which means that you can directly use the quantized model with <code>transformers</code>. The following is a very simple code snippet showing how to run <code>Qwen2.5-7B-Instruct-AWQ</code> with the quantized model:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model_name </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Qwen/Qwen2.5-7B-Instruct-AWQ&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_name, </span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">prompt </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Give me a short introduction to large language model.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">messages </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: prompt},</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color:#E1E4E8;">    messages,</span></span>
<span class="line"><span style="color:#FFAB70;">    tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model_inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer([text], </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">).to(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#F97583;">    **</span><span style="color:#E1E4E8;">model_inputs,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    output_ids[</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(input_ids):] </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> input_ids, output_ids </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(model_inputs.input_ids, generated_ids)</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.batch_decode(generated_ids, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span></code></pre></div><h2 id="usage-of-awq-models-with-vllm" tabindex="-1">Usage of AWQ Models with vLLM <a class="header-anchor" href="#usage-of-awq-models-with-vllm" aria-label="Permalink to &quot;Usage of AWQ  Models with vLLM&quot;">​</a></h2><p>vLLM has supported AWQ, which means that you can directly use our provided AWQ models or those quantized with <code>AutoAWQ</code> with vLLM. We recommend using the latest version of vLLM (<code>vllm&gt;=0.6.1</code>) which brings performance improvements to AWQ models; otherwise, the performance might not be well-optimized.</p><p>Actually, the usage is the same with the basic usage of vLLM. We provide a simple example of how to launch OpenAI-API compatible API with vLLM and <code>Qwen2.5-7B-Instruct-AWQ</code>:</p><p>Run the following in a shell to start an OpenAI-compatible API service:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">vllm</span><span style="color:#9ECBFF;"> serve</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-7B-Instruct-AWQ</span></span></code></pre></div><p>Then, you can call the API as</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">curl</span><span style="color:#9ECBFF;"> http://localhost:8000/v1/chat/completions</span><span style="color:#79B8FF;"> -H</span><span style="color:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="color:#79B8FF;"> -d</span><span style="color:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct-AWQ&quot;,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;messages&quot;: [</span></span>
<span class="line"><span style="color:#9ECBFF;">    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;},</span></span>
<span class="line"><span style="color:#9ECBFF;">    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me something about large language models.&quot;}</span></span>
<span class="line"><span style="color:#9ECBFF;">  ],</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;temperature&quot;: 0.7,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;top_p&quot;: 0.8,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;repetition_penalty&quot;: 1.05,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;max_tokens&quot;: 512</span></span>
<span class="line"><span style="color:#9ECBFF;">}&#39;</span></span></code></pre></div><p>or you can use the API client with the <code>openai</code> Python package as shown below:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> openai </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> OpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">openai_api_key </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;EMPTY&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">openai_api_base </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;http://localhost:8000/v1&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">client </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> OpenAI(</span></span>
<span class="line"><span style="color:#FFAB70;">    api_key</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">openai_api_key,</span></span>
<span class="line"><span style="color:#FFAB70;">    base_url</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">openai_api_base,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">chat_response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> client.chat.completions.create(</span></span>
<span class="line"><span style="color:#FFAB70;">    model</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;Qwen/Qwen2.5-7B-Instruct-AWQ&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    messages</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Tell me something about large language models.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    ],</span></span>
<span class="line"><span style="color:#FFAB70;">    temperature</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.7</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    top_p</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.8</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    extra_body</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">{</span></span>
<span class="line"><span style="color:#9ECBFF;">        &quot;repetition_penalty&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">1.05</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">    },</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&quot;Chat response:&quot;</span><span style="color:#E1E4E8;">, chat_response)</span></span></code></pre></div><h2 id="quantize-your-own-model-with-autoawq" tabindex="-1">Quantize Your Own Model with AutoAWQ <a class="header-anchor" href="#quantize-your-own-model-with-autoawq" aria-label="Permalink to &quot;Quantize Your Own Model with AutoAWQ&quot;">​</a></h2><p>If you want to quantize your own model to AWQ quantized models, we advise you to use AutoAWQ.</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> &quot;autoawq&lt;0.2.7&quot;</span></span></code></pre></div><p>Suppose you have finetuned a model based on <code>Qwen2.5-7B</code>, which is named <code>Qwen2.5-7B-finetuned</code>, with your own dataset, e.g., Alpaca. To build your own AWQ quantized model, you need to use the training data for calibration. Below, we provide a simple demonstration for you to run:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> awq </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoAWQForCausalLM</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Specify paths and hyperparameters for quantization</span></span>
<span class="line"><span style="color:#E1E4E8;">model_path </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;your_model_path&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">quant_path </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;your_quantized_model_path&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">quant_config </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> { </span><span style="color:#9ECBFF;">&quot;zero_point&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;q_group_size&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">128</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;w_bit&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;version&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;GEMM&quot;</span><span style="color:#E1E4E8;"> }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Load your tokenizer and model with AutoAWQ</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(model_path)</span></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoAWQForCausalLM.from_pretrained(model_path, </span><span style="color:#FFAB70;">device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">safetensors</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>Then you need to prepare your data for calibration. What you need to do is just put samples into a list, each of which is a text. As we directly use our finetuning data for calibration, we first format it with ChatML template. For example,</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">data </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> msg </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> dataset:</span></span>
<span class="line"><span style="color:#E1E4E8;">    text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(msg, </span><span style="color:#FFAB70;">tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    data.append(text.strip())</span></span></code></pre></div><p>where each <code>msg</code> is a typical chat message as shown below:</p><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">[</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#79B8FF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#79B8FF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Tell me who you are.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#79B8FF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;assistant&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;I am a large language model named Qwen...&quot;</span><span style="color:#E1E4E8;">}</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span></code></pre></div><p>Then just run the calibration process by one line of code:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">model.quantize(tokenizer, </span><span style="color:#FFAB70;">quant_config</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">quant_config, </span><span style="color:#FFAB70;">calib_data</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">data)</span></span></code></pre></div><p>Finally, save the quantized model:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">model.save_quantized(quant_path, </span><span style="color:#FFAB70;">safetensors</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">shard_size</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;4GB&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer.save_pretrained(quant_path)</span></span></code></pre></div><p>Then you can obtain your own AWQ quantized model for deployment. Enjoy!</p>`,31)]))}const d=a(l,[["render",p]]);export{u as __pageData,d as default};
