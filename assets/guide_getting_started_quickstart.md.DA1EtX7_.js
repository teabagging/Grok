import{_ as a,c as n,b as e,o}from"./chunks/framework.B1z0IdBH.js";const d=JSON.parse('{"title":"Quickstart","description":"","frontmatter":{},"headers":[{"level":2,"title":"Hugging Face Transformers & ModelScope","slug":"hugging-face-transformers-modelscope","link":"#hugging-face-transformers-modelscope","children":[{"level":3,"title":"Streaming Generation","slug":"streaming-generation","link":"#streaming-generation","children":[]},{"level":3,"title":"ModelScope","slug":"modelscope","link":"#modelscope","children":[]}]},{"level":2,"title":"vLLM for Deployment","slug":"vllm-for-deployment","link":"#vllm-for-deployment","children":[]},{"level":2,"title":"Next Step","slug":"next-step","link":"#next-step","children":[]}],"relativePath":"guide/getting_started/quickstart.md","filePath":"guide/getting_started/quickstart.md"}'),l={name:"guide/getting_started/quickstart.md"};function p(t,s,r,c,i,E){return o(),n("div",null,s[0]||(s[0]=[e(`<h1 id="quickstart" tabindex="-1">Quickstart <a class="header-anchor" href="#quickstart" aria-label="Permalink to &quot;Quickstart&quot;">​</a></h1><p>This guide helps you quickly start using Qwen2.5. We provide examples of <a href="https://github.com/huggingface/transformers" target="_blank" rel="noreferrer">Hugging Face Transformers</a> as well as <a href="https://github.com/modelscope/modelscope" target="_blank" rel="noreferrer">ModelScope</a>, and <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noreferrer">vLLM</a> for deployment.</p><p>You can find Qwen2.5 models in the <a href="https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e" target="_blank" rel="noreferrer">Qwen2.5 collection</a> at Hugging Face Hub.</p><h2 id="hugging-face-transformers-modelscope" tabindex="-1">Hugging Face Transformers &amp; ModelScope <a class="header-anchor" href="#hugging-face-transformers-modelscope" aria-label="Permalink to &quot;Hugging Face Transformers &amp; ModelScope&quot;">​</a></h2><p>To get a quick start with Qwen2.5, we advise you to try with the inference with <code>transformers</code> first. Make sure that you have installed <code>transformers&gt;=4.37.0</code>. We advise you to use Python 3.10 or higher, and PyTorch 2.3 or higher.</p><p>:::{dropdown} Install <code>transformers</code></p><ul><li><p>Install with <code>pip</code>:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> transformers</span><span style="color:#79B8FF;"> -U</span></span></code></pre></div></li><li><p>Install with <code>conda</code>:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> conda-forge::transformers</span></span></code></pre></div></li><li><p>Install from source:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> git+https://github.com/huggingface/transformers</span></span></code></pre></div></li></ul><p>:::</p><p>The following is a very simple code snippet showing how to run Qwen2.5-7B-Instruct:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model_name </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Qwen/Qwen2.5-7B-Instruct&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_name,</span></span>
<span class="line"><span style="color:#FFAB70;">    torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">prompt </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Give me a short introduction to large language model.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">messages </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: prompt},</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color:#E1E4E8;">    messages,</span></span>
<span class="line"><span style="color:#FFAB70;">    tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model_inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer([text], </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">).to(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#F97583;">    **</span><span style="color:#E1E4E8;">model_inputs,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    output_ids[</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(input_ids):] </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> input_ids, output_ids </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(model_inputs.input_ids, generated_ids)</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.batch_decode(generated_ids, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span></code></pre></div><p>As you can see, it&#39;s just standard usage for casual LMs in <code>transformers</code>!</p><h3 id="streaming-generation" tabindex="-1">Streaming Generation <a class="header-anchor" href="#streaming-generation" aria-label="Permalink to &quot;Streaming Generation&quot;">​</a></h3><p>Streaming mode for model chat is simple with the help of <code>TextStreamer</code>. Below we show you an example of how to use it:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#79B8FF;">...</span></span>
<span class="line"><span style="color:#6A737D;"># Reuse the code before \`model.generate()\` in the last code snippet</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> TextStreamer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">streamer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> TextStreamer(tokenizer, </span><span style="color:#FFAB70;">skip_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#F97583;">    **</span><span style="color:#E1E4E8;">model_inputs,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    streamer</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">streamer,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>It will print the text to the console or the terminal as being generated.</p><h3 id="modelscope" tabindex="-1">ModelScope <a class="header-anchor" href="#modelscope" aria-label="Permalink to &quot;ModelScope&quot;">​</a></h3><p>To tackle with downloading issues, we advise you to try <a href="https://github.com/modelscope/modelscope" target="_blank" rel="noreferrer">ModelScope</a>. Before starting, you need to install <code>modelscope</code> with <code>pip</code>.</p><p><code>modelscope</code> adopts a programmatic interface similar (but not identical) to <code>transformers</code>. For basic usage, you can simply change the first line of code above to the following:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> modelscope </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer</span></span></code></pre></div><p>For more information, please refer to <a href="https://www.modelscope.cn/docs" target="_blank" rel="noreferrer">the documentation of <code>modelscope</code></a>.</p><h2 id="vllm-for-deployment" tabindex="-1">vLLM for Deployment <a class="header-anchor" href="#vllm-for-deployment" aria-label="Permalink to &quot;vLLM for Deployment&quot;">​</a></h2><p>To deploy Qwen2.5, we advise you to use vLLM. vLLM is a fast and easy-to-use framework for LLM inference and serving. In the following, we demonstrate how to build a OpenAI-API compatible API service with vLLM.</p><p>First, make sure you have installed <code>vllm&gt;=0.4.0</code>:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> vllm</span></span></code></pre></div><p>Run the following code to build up a vLLM service. Here we take Qwen2.5-7B-Instruct as an example:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#79B8FF;"> -m</span><span style="color:#9ECBFF;"> vllm.entrypoints.openai.api_server</span><span style="color:#79B8FF;"> --model</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-7B-Instruct</span></span></code></pre></div><p>with <code>vllm&gt;=0.5.3</code>, you can also use</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">vllm</span><span style="color:#9ECBFF;"> serve</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-7B-Instruct</span></span></code></pre></div><p>Then, you can use the <a href="https://platform.openai.com/docs/api-reference/chat/completions/create" target="_blank" rel="noreferrer">create chat interface</a> to communicate with Qwen:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">curl</span><span style="color:#9ECBFF;"> http://localhost:8000/v1/chat/completions</span><span style="color:#79B8FF;"> -H</span><span style="color:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="color:#79B8FF;"> -d</span><span style="color:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;messages&quot;: [</span></span>
<span class="line"><span style="color:#9ECBFF;">    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;},</span></span>
<span class="line"><span style="color:#9ECBFF;">    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me something about large language models.&quot;}</span></span>
<span class="line"><span style="color:#9ECBFF;">  ],</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;temperature&quot;: 0.7,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;top_p&quot;: 0.8,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;repetition_penalty&quot;: 1.05,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;max_tokens&quot;: 512</span></span>
<span class="line"><span style="color:#9ECBFF;">}&#39;</span></span></code></pre></div><p>or you can use Python client with <code>openai</code> Python package as shown below:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> openai </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> OpenAI</span></span>
<span class="line"><span style="color:#6A737D;"># Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.</span></span>
<span class="line"><span style="color:#E1E4E8;">openai_api_key </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;EMPTY&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">openai_api_base </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;http://localhost:8000/v1&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">client </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> OpenAI(</span></span>
<span class="line"><span style="color:#FFAB70;">    api_key</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">openai_api_key,</span></span>
<span class="line"><span style="color:#FFAB70;">    base_url</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">openai_api_base,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">chat_response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> client.chat.completions.create(</span></span>
<span class="line"><span style="color:#FFAB70;">    model</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    messages</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Tell me something about large language models.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    ],</span></span>
<span class="line"><span style="color:#FFAB70;">    temperature</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.7</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    top_p</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.8</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    extra_body</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">{</span></span>
<span class="line"><span style="color:#9ECBFF;">        &quot;repetition_penalty&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">1.05</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">    },</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&quot;Chat response:&quot;</span><span style="color:#E1E4E8;">, chat_response)</span></span></code></pre></div><p>For more information, please refer to <a href="https://docs.vllm.ai/en/stable/" target="_blank" rel="noreferrer">the documentation of <code>vllm</code></a>.</p><h2 id="next-step" tabindex="-1">Next Step <a class="header-anchor" href="#next-step" aria-label="Permalink to &quot;Next Step&quot;">​</a></h2><p>Now, you can have fun with Qwen2.5 models. Would love to know more about its usages? Feel free to check other documents in this documentation.</p>`,35)]))}const u=a(l,[["render",p]]);export{d as __pageData,u as default};
