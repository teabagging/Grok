import{_ as a,c as n,b as e,o as l}from"./chunks/framework.B1z0IdBH.js";const y=JSON.parse('{"title":"效率评估","description":"","frontmatter":{},"headers":[{"level":2,"title":"1. 模型资源","slug":"_1-模型资源","link":"#_1-模型资源","children":[]},{"level":2,"title":"2. 环境安装","slug":"_2-环境安装","link":"#_2-环境安装","children":[]},{"level":2,"title":"3. 执行测试","slug":"_3-执行测试","link":"#_3-执行测试","children":[{"level":3,"title":"方法1：使用Speed Benchmark工具测试","slug":"方法1-使用speed-benchmark工具测试","link":"#方法1-使用speed-benchmark工具测试","children":[]},{"level":3,"title":"方法2：使用脚本测试","slug":"方法2-使用脚本测试","link":"#方法2-使用脚本测试","children":[]}]},{"level":2,"title":"注意事项","slug":"注意事项","link":"#注意事项","children":[]}],"relativePath":"guide/speed-benchmark/README_zh.md","filePath":"guide/speed-benchmark/README_zh.md"}'),o={name:"guide/speed-benchmark/README_zh.md"};function p(t,s,r,c,F,i){return l(),n("div",null,s[0]||(s[0]=[e('<h1 id="效率评估" tabindex="-1">效率评估 <a class="header-anchor" href="#效率评估" aria-label="Permalink to &quot;效率评估&quot;">​</a></h1><p>本文介绍Qwen2.5系列模型（原始模型和量化模型）的效率测试流程，详细报告可参考 <a href="https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html" target="_blank" rel="noreferrer">Qwen2.5模型效率评估报告</a>。</p><h2 id="_1-模型资源" tabindex="-1">1. 模型资源 <a class="header-anchor" href="#_1-模型资源" aria-label="Permalink to &quot;1. 模型资源&quot;">​</a></h2><p>对于托管在HuggingFace上的模型，可参考 <a href="https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e" target="_blank" rel="noreferrer">Qwen2.5模型-HuggingFace</a>。</p><p>对于托管在ModelScope上的模型，可参考 <a href="https://modelscope.cn/collections/Qwen25-dbc4d30adb768" target="_blank" rel="noreferrer">Qwen2.5模型-ModelScope</a>。</p><h2 id="_2-环境安装" tabindex="-1">2. 环境安装 <a class="header-anchor" href="#_2-环境安装" aria-label="Permalink to &quot;2. 环境安装&quot;">​</a></h2><p>使用HuggingFace transformers推理，安装环境如下：</p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> create</span><span style="color:#79B8FF;"> -n</span><span style="color:#9ECBFF;"> qwen_perf_transformers</span><span style="color:#9ECBFF;"> python=</span><span style="color:#79B8FF;">3.10</span></span>\n<span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> activate</span><span style="color:#9ECBFF;"> qwen_perf_transformers</span></span>\n<span class="line"></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> torch==</span><span style="color:#79B8FF;">2.3.1</span></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> git+https://github.com/AutoGPTQ/AutoGPTQ.git@v0.7.1</span></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> git+https://github.com/Dao-AILab/flash-attention.git@v2.5.8</span></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#79B8FF;"> -r</span><span style="color:#9ECBFF;"> requirements-perf-transformers.txt</span></span></code></pre></div><div class="important custom-block github-alert"><p class="custom-block-title">IMPORTANT</p><p></p><ul><li>对于 <code>flash-attention</code>，您可以从 <a href="https://github.com/Dao-AILab/flash-attention/releases/tag/v2.5.8" target="_blank" rel="noreferrer">GitHub 发布页面</a> 使用预编译的 wheel 包进行安装，或者从源代码安装，后者需要一个兼容的 CUDA 编译器。 <ul><li>实际上，您并不需要单独安装 <code>flash-attention</code>。它已经被集成到了 <code>torch</code> 中作为 <code>sdpa</code> 的后端实现。</li></ul></li><li>若要使 <code>auto_gptq</code> 使用高效的内核，您需要从源代码安装，因为预编译的 wheel 包依赖于与之不兼容的 <code>torch</code> 版本。从源代码安装同样需要一个兼容的 CUDA 编译器。</li><li>若要使 <code>autoawq</code> 使用高效的内核，您需要安装 <code>autoawq-kernels</code>，该组件应当会自动安装。如果未自动安装，请运行 <code>pip install autoawq-kernels</code> 进行手动安装。</li></ul></div><p>使用vLLM推理，安装环境如下：</p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> create</span><span style="color:#79B8FF;"> -n</span><span style="color:#9ECBFF;"> qwen_perf_vllm</span><span style="color:#9ECBFF;"> python=</span><span style="color:#79B8FF;">3.10</span></span>\n<span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> activate</span><span style="color:#9ECBFF;"> qwen_perf_vllm</span></span>\n<span class="line"></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#79B8FF;"> -r</span><span style="color:#9ECBFF;"> requirements-perf-vllm.txt</span></span></code></pre></div><h2 id="_3-执行测试" tabindex="-1">3. 执行测试 <a class="header-anchor" href="#_3-执行测试" aria-label="Permalink to &quot;3. 执行测试&quot;">​</a></h2><p>下面介绍两种执行测试的方法，分别是使用脚本测试和使用Speed Benchmark工具进行测试。</p><h3 id="方法1-使用speed-benchmark工具测试" tabindex="-1">方法1：使用Speed Benchmark工具测试 <a class="header-anchor" href="#方法1-使用speed-benchmark工具测试" aria-label="Permalink to &quot;方法1：使用Speed Benchmark工具测试&quot;">​</a></h3><p>使用<a href="https://github.com/modelscope/evalscope" target="_blank" rel="noreferrer">EvalScope</a>开发的Speed Benchmark工具进行测试，支持自动从modelscope下载模型并输出测试结果，也支持指定模型服务的url进行测试，具体请参考<a href="https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/speed_benchmark.html" target="_blank" rel="noreferrer">📖使用指南</a>。</p><p><strong>安装依赖</strong></p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> &#39;evalscope[perf]&#39;</span><span style="color:#79B8FF;"> -U</span></span></code></pre></div><h4 id="huggingface-transformers推理" tabindex="-1">HuggingFace transformers推理 <a class="header-anchor" href="#huggingface-transformers推理" aria-label="Permalink to &quot;HuggingFace transformers推理&quot;">​</a></h4><p>执行命令如下：</p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">CUDA_VISIBLE_DEVICES</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">0</span><span style="color:#B392F0;"> evalscope</span><span style="color:#9ECBFF;"> perf</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --parallel</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --model</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --attn-implementation</span><span style="color:#9ECBFF;"> flash_attention_2</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --log-every-n-query</span><span style="color:#79B8FF;"> 5</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --connect-timeout</span><span style="color:#79B8FF;"> 6000</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --read-timeout</span><span style="color:#79B8FF;"> 6000</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --max-tokens</span><span style="color:#79B8FF;"> 2048</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --min-tokens</span><span style="color:#79B8FF;"> 2048</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --api</span><span style="color:#9ECBFF;"> local</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --dataset</span><span style="color:#9ECBFF;"> speed_benchmark</span></span></code></pre></div><h4 id="vllm推理" tabindex="-1">vLLM推理 <a class="header-anchor" href="#vllm推理" aria-label="Permalink to &quot;vLLM推理&quot;">​</a></h4><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">CUDA_VISIBLE_DEVICES</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">0</span><span style="color:#B392F0;"> evalscope</span><span style="color:#9ECBFF;"> perf</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --parallel</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --model</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --log-every-n-query</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --connect-timeout</span><span style="color:#79B8FF;"> 60000</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --read-timeout</span><span style="color:#79B8FF;"> 60000</span><span style="color:#79B8FF;">\\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --max-tokens</span><span style="color:#79B8FF;"> 2048</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --min-tokens</span><span style="color:#79B8FF;"> 2048</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --api</span><span style="color:#9ECBFF;"> local_vllm</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --dataset</span><span style="color:#9ECBFF;"> speed_benchmark</span></span></code></pre></div><h4 id="参数说明" tabindex="-1">参数说明 <a class="header-anchor" href="#参数说明" aria-label="Permalink to &quot;参数说明&quot;">​</a></h4><ul><li><code>--parallel</code> 设置并发请求的worker数量，需固定为1。</li><li><code>--model</code> 测试模型文件路径，也可为模型ID，支持自动从modelscope下载模型，例如Qwen/Qwen2.5-0.5B-Instruct。</li><li><code>--attn-implementation</code> 设置attention实现方式，可选值为flash_attention_2|eager|sdpa。</li><li><code>--log-every-n-query</code>: 设置每n个请求打印一次日志。</li><li><code>--connect-timeout</code>: 设置连接超时时间，单位为秒。</li><li><code>--read-timeout</code>: 设置读取超时时间，单位为秒。</li><li><code>--max-tokens</code>: 设置最大输出长度，单位为token。</li><li><code>--min-tokens</code>: 设置最小输出长度，单位为token；两个参数同时设置为2048则模型固定输出长度为2048。</li><li><code>--api</code>: 设置推理接口，本地推理可选值为local|local_vllm。</li><li><code>--dataset</code>: 设置测试数据集，可选值为speed_benchmark|speed_benchmark_long。</li></ul><h4 id="测试结果" tabindex="-1">测试结果 <a class="header-anchor" href="#测试结果" aria-label="Permalink to &quot;测试结果&quot;">​</a></h4><p>测试结果详见<code>outputs/{model_name}/{timestamp}/speed_benchmark.json</code>文件，其中包含所有请求结果和测试参数。</p><h3 id="方法2-使用脚本测试" tabindex="-1">方法2：使用脚本测试 <a class="header-anchor" href="#方法2-使用脚本测试" aria-label="Permalink to &quot;方法2：使用脚本测试&quot;">​</a></h3><h4 id="huggingface-transformers推理-1" tabindex="-1">HuggingFace transformers推理 <a class="header-anchor" href="#huggingface-transformers推理-1" aria-label="Permalink to &quot;HuggingFace transformers推理&quot;">​</a></h4><ul><li>使用HuggingFace hub</li></ul><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#9ECBFF;"> speed_benchmark_transformers.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/transformers</span></span>\n<span class="line"></span>\n<span class="line"><span style="color:#6A737D;"># 指定HF_ENDPOINT</span></span>\n<span class="line"><span style="color:#E1E4E8;">HF_ENDPOINT</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">https://hf-mirror.com</span><span style="color:#B392F0;"> python</span><span style="color:#9ECBFF;"> speed_benchmark_transformers.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/transformers</span></span></code></pre></div><ul><li>使用ModelScope hub</li></ul><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#9ECBFF;"> speed_benchmark_transformers.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --use_modelscope</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/transformers</span></span></code></pre></div><p>参数说明：</p><pre><code>`--model_id_or_path`: 模型ID或本地路径， 可选值参考`模型资源`章节  \n`--context_length`: 输入长度，单位为token数；可选值为1, 6144, 14336, 30720, 63488, 129024；具体可参考`Qwen2.5模型效率评估报告`  \n`--generate_length`: 生成token数量；默认为2048\n`--gpus`: 等价于环境变量CUDA_VISIBLE_DEVICES，例如`0,1,2,3`，`4,5`  \n`--use_modelscope`: 如果设置该值，则使用ModelScope加载模型，否则使用HuggingFace  \n`--outputs_dir`: 输出目录， 默认为`outputs/transformers`  \n</code></pre><h4 id="vllm推理-1" tabindex="-1">vLLM推理 <a class="header-anchor" href="#vllm推理-1" aria-label="Permalink to &quot;vLLM推理&quot;">​</a></h4><ul><li>使用HuggingFace hub</li></ul><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#9ECBFF;"> speed_benchmark_vllm.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --max_model_len</span><span style="color:#79B8FF;"> 32768</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --gpu_memory_utilization</span><span style="color:#79B8FF;"> 0.9</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/vllm</span></span>\n<span class="line"></span>\n<span class="line"><span style="color:#6A737D;"># 指定HF_ENDPOINT</span></span>\n<span class="line"><span style="color:#E1E4E8;">HF_ENDPOINT</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">https://hf-mirror.com</span><span style="color:#B392F0;"> python</span><span style="color:#9ECBFF;"> speed_benchmark_vllm.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --max_model_len</span><span style="color:#79B8FF;"> 32768</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --gpu_memory_utilization</span><span style="color:#79B8FF;"> 0.9</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/vllm</span></span></code></pre></div><ul><li>使用ModelScope hub</li></ul><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#9ECBFF;"> speed_benchmark_vllm.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --max_model_len</span><span style="color:#79B8FF;"> 32768</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --use_modelscope</span><span style="color:#79B8FF;"> --gpu_memory_utilization</span><span style="color:#79B8FF;"> 0.9</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/vllm</span></span></code></pre></div><p>参数说明：</p><pre><code>`--model_id_or_path`: 模型ID或本地路径， 可选值参考`模型资源`章节  \n`--context_length`: 输入长度，单位为token数；可选值为1, 6144, 14336, 30720, 63488, 129024；具体可参考`Qwen2.5模型效率评估报告`  \n`--generate_length`: 生成token数量；默认为2048\n`--max_model_len`: 模型最大长度，单位为token数；默认为32768  \n`--gpus`: 等价于环境变量CUDA_VISIBLE_DEVICES，例如`0,1,2,3`，`4,5`   \n`--use_modelscope`: 如果设置该值，则使用ModelScope加载模型，否则使用HuggingFace  \n`--gpu_memory_utilization`: GPU内存利用率，取值范围为(0, 1]；默认为0.9  \n`--outputs_dir`: 输出目录， 默认为`outputs/vllm`  \n`--enforce_eager`: 是否强制使用eager模式；默认为False  \n</code></pre><h4 id="测试结果-1" tabindex="-1">测试结果 <a class="header-anchor" href="#测试结果-1" aria-label="Permalink to &quot;测试结果&quot;">​</a></h4><p>测试结果详见<code>outputs</code>目录下的文件，默认包括<code>transformers</code>和<code>vllm</code>两个目录，分别存放HuggingFace transformers和vLLM的测试结果。</p><h2 id="注意事项" tabindex="-1">注意事项 <a class="header-anchor" href="#注意事项" aria-label="Permalink to &quot;注意事项&quot;">​</a></h2><ol><li>多次测试，取平均值，典型值为3次</li><li>测试前请确保GPU处于空闲状态，避免其他任务影响测试结果</li></ol>',45)]))}const h=a(o,[["render",p]]);export{y as __pageData,h as default};
