import{_ as a,c as o,b as n,a as e,o as l}from"./chunks/framework.B1z0IdBH.js";const d=JSON.parse('{"title":"GPTQ","description":"","frontmatter":{},"headers":[{"level":2,"title":"Usage of GPTQ Models with Hugging Face transformers","slug":"usage-of-gptq-models-with-hugging-face-transformers","link":"#usage-of-gptq-models-with-hugging-face-transformers","children":[]},{"level":2,"title":"Usage of GPTQ Models with vLLM","slug":"usage-of-gptq-models-with-vllm","link":"#usage-of-gptq-models-with-vllm","children":[]},{"level":2,"title":"Quantize Your Own Model with AutoGPTQ","slug":"quantize-your-own-model-with-autogptq","link":"#quantize-your-own-model-with-autogptq","children":[]},{"level":2,"title":"Known Issues","slug":"known-issues","link":"#known-issues","children":[{"level":3,"title":"Qwen2.5-72B-Instruct-GPTQ-Int4 cannot stop generation properly","slug":"qwen2-5-72b-instruct-gptq-int4-cannot-stop-generation-properly","link":"#qwen2-5-72b-instruct-gptq-int4-cannot-stop-generation-properly","children":[]},{"level":3,"title":"Qwen2.5-32B-Instruct-GPTQ-Int4 broken with vLLM on multiple GPUs","slug":"qwen2-5-32b-instruct-gptq-int4-broken-with-vllm-on-multiple-gpus","link":"#qwen2-5-32b-instruct-gptq-int4-broken-with-vllm-on-multiple-gpus","children":[]}]},{"level":2,"title":"Troubleshooting","slug":"troubleshooting","link":"#troubleshooting","children":[]}],"relativePath":"guide/quantization/gptq.md","filePath":"guide/quantization/gptq.md"}'),p={name:"guide/quantization/gptq.md"};function t(r,s,c,i,E,y){return l(),o("div",null,s[0]||(s[0]=[n('<h1 id="gptq" tabindex="-1">GPTQ <a class="header-anchor" href="#gptq" aria-label="Permalink to &quot;GPTQ&quot;">​</a></h1><p><a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noreferrer">GPTQ</a> is a quantization method for GPT-like LLMs, which uses one-shot weight quantization based on approximate second-order information. In this document, we show you how to use the quantized model with Hugging Face <code>transformers</code> and also how to quantize your own model with <a href="https://github.com/AutoGPTQ/AutoGPTQ" target="_blank" rel="noreferrer">AutoGPTQ</a>.</p><h2 id="usage-of-gptq-models-with-hugging-face-transformers" tabindex="-1">Usage of GPTQ Models with Hugging Face transformers <a class="header-anchor" href="#usage-of-gptq-models-with-hugging-face-transformers" aria-label="Permalink to &quot;Usage of GPTQ Models with Hugging Face transformers&quot;">​</a></h2>',3),e("p",{note:""},":::",-1),n(`<p>To use the official Qwen2.5 GPTQ models with <code>transformers</code>, please ensure that <code>optimum&gt;=1.20.0</code> and compatible versions of <code>transformers</code> and <code>auto_gptq</code> are installed.</p><p>You can do that by</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#79B8FF;"> -U</span><span style="color:#9ECBFF;"> &quot;optimum&gt;=1.20.0&quot;</span></span></code></pre></div><p>:::</p><p>Now, <code>transformers</code> has officially supported AutoGPTQ, which means that you can directly use the quantized model with <code>transformers</code>. For each size of Qwen2.5, we provide both Int4 and Int8 GPTQ quantized models. The following is a very simple code snippet showing how to run <code>Qwen2.5-7B-Instruct-GPTQ-Int4</code>:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model_name </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_name, </span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">prompt </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Give me a short introduction to large language model.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">messages </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: prompt},</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color:#E1E4E8;">    messages,</span></span>
<span class="line"><span style="color:#FFAB70;">    tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model_inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer([text], </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">).to(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#F97583;">    **</span><span style="color:#E1E4E8;">model_inputs,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    output_ids[</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(input_ids):] </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> input_ids, output_ids </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(model_inputs.input_ids, generated_ids)</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.batch_decode(generated_ids, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span></code></pre></div><h2 id="usage-of-gptq-models-with-vllm" tabindex="-1">Usage of GPTQ Models with vLLM <a class="header-anchor" href="#usage-of-gptq-models-with-vllm" aria-label="Permalink to &quot;Usage of GPTQ Models with vLLM&quot;">​</a></h2><p>vLLM has supported GPTQ, which means that you can directly use our provided GPTQ models or those trained with <code>AutoGPTQ</code> with vLLM. If possible, it will automatically use the GPTQ Marlin kernel, which is more efficient.</p><p>Actually, the usage is the same with the basic usage of vLLM. We provide a simple example of how to launch OpenAI-API compatible API with vLLM and <code>Qwen2.5-7B-Instruct-GPTQ-Int4</code>:</p><p>Run the following in a shell to start an OpenAI-compatible API service:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">vllm</span><span style="color:#9ECBFF;"> serve</span><span style="color:#9ECBFF;"> Qwen2.5-7B-Instruct-GPTQ-Int4</span></span></code></pre></div><p>Then, you can call the API as</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">curl</span><span style="color:#9ECBFF;"> http://localhost:8000/v1/chat/completions</span><span style="color:#79B8FF;"> -H</span><span style="color:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="color:#79B8FF;"> -d</span><span style="color:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;model&quot;: &quot;Qwen2.5-7B-Instruct-GPTQ-Int4&quot;,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;messages&quot;: [</span></span>
<span class="line"><span style="color:#9ECBFF;">    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;},</span></span>
<span class="line"><span style="color:#9ECBFF;">    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me something about large language models.&quot;}</span></span>
<span class="line"><span style="color:#9ECBFF;">  ],</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;temperature&quot;: 0.7,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;top_p&quot;: 0.8,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;repetition_penalty&quot;: 1.05,</span></span>
<span class="line"><span style="color:#9ECBFF;">  &quot;max_tokens&quot;: 512</span></span>
<span class="line"><span style="color:#9ECBFF;">}&#39;</span></span></code></pre></div><p>or you can use the API client with the <code>openai</code> Python package as shown below:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> openai </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> OpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">openai_api_key </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;EMPTY&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">openai_api_base </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;http://localhost:8000/v1&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">client </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> OpenAI(</span></span>
<span class="line"><span style="color:#FFAB70;">    api_key</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">openai_api_key,</span></span>
<span class="line"><span style="color:#FFAB70;">    base_url</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">openai_api_base,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">chat_response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> client.chat.completions.create(</span></span>
<span class="line"><span style="color:#FFAB70;">    model</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;Qwen2.5-7B-Instruct-GPTQ-Int4&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    messages</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Tell me something about large language models.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    ],</span></span>
<span class="line"><span style="color:#FFAB70;">    temperature</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.7</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    top_p</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.8</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    extra_body</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">{</span></span>
<span class="line"><span style="color:#9ECBFF;">        &quot;repetition_penalty&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">1.05</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">    },</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&quot;Chat response:&quot;</span><span style="color:#E1E4E8;">, chat_response)</span></span></code></pre></div><h2 id="quantize-your-own-model-with-autogptq" tabindex="-1">Quantize Your Own Model with AutoGPTQ <a class="header-anchor" href="#quantize-your-own-model-with-autogptq" aria-label="Permalink to &quot;Quantize Your Own Model with AutoGPTQ&quot;">​</a></h2><p>If you want to quantize your own model to GPTQ quantized models, we advise you to use AutoGPTQ. It is suggested installing the latest version of the package by installing from source code:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">git</span><span style="color:#9ECBFF;"> clone</span><span style="color:#9ECBFF;"> https://github.com/AutoGPTQ/AutoGPTQ</span></span>
<span class="line"><span style="color:#79B8FF;">cd</span><span style="color:#9ECBFF;"> AutoGPTQ</span></span>
<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#79B8FF;"> -e</span><span style="color:#9ECBFF;"> .</span></span></code></pre></div><p>Suppose you have finetuned a model based on <code>Qwen2.5-7B</code>, which is named <code>Qwen2.5-7B-finetuned</code>, with your own dataset, e.g., Alpaca. To build your own GPTQ quantized model, you need to use the training data for calibration. Below, we provide a simple demonstration for you to run:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> auto_gptq </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoGPTQForCausalLM, BaseQuantizeConfig</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Specify paths and hyperparameters for quantization</span></span>
<span class="line"><span style="color:#E1E4E8;">model_path </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;your_model_path&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">quant_path </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;your_quantized_model_path&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">quantize_config </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> BaseQuantizeConfig(</span></span>
<span class="line"><span style="color:#FFAB70;">    bits</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">, </span><span style="color:#6A737D;"># 4 or 8</span></span>
<span class="line"><span style="color:#FFAB70;">    group_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">128</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    damp_percent</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.01</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    desc_act</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,  </span><span style="color:#6A737D;"># set to False can significantly speed up inference but the perplexity may slightly bad</span></span>
<span class="line"><span style="color:#FFAB70;">    static_groups</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    sym</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    true_sequential</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    model_name_or_path</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    model_file_base_name</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;model&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">max_len </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 8192</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Load your tokenizer and model with AutoGPTQ</span></span>
<span class="line"><span style="color:#6A737D;"># To learn about loading model to multiple GPUs,</span></span>
<span class="line"><span style="color:#6A737D;"># visit https://github.com/AutoGPTQ/AutoGPTQ/blob/main/docs/tutorial/02-Advanced-Model-Loading-and-Best-Practice.md</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(model_path)</span></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)</span></span></code></pre></div><p>However, if you would like to load the model on multiple GPUs, you need to use <code>max_memory</code> instead of <code>device_map</code>. Here is an example:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoGPTQForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_path,</span></span>
<span class="line"><span style="color:#E1E4E8;">    quantize_config,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_memory</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">{i: </span><span style="color:#9ECBFF;">&quot;20GB&quot;</span><span style="color:#F97583;"> for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">)}</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>Then you need to prepare your data for calibration. What you need to do is just put samples into a list, each of which is a text. As we directly use our finetuning data for calibration, we first format it with ChatML template. For example,</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">data </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> msg </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> dataset:</span></span>
<span class="line"><span style="color:#E1E4E8;">    text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(msg, </span><span style="color:#FFAB70;">tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer([text])</span></span>
<span class="line"><span style="color:#E1E4E8;">    input_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor(model_inputs.input_ids[:max_len], </span><span style="color:#FFAB70;">dtype</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">torch.int)</span></span>
<span class="line"><span style="color:#E1E4E8;">    data.append(</span><span style="color:#79B8FF;">dict</span><span style="color:#E1E4E8;">(</span><span style="color:#FFAB70;">input_ids</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">input_ids, </span><span style="color:#FFAB70;">attention_mask</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">input_ids.ne(tokenizer.pad_token_id)))</span></span></code></pre></div><p>where each <code>msg</code> is a typical chat message as shown below:</p><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">[</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#79B8FF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#79B8FF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Tell me who you are.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#79B8FF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;assistant&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;I am a large language model named Qwen...&quot;</span><span style="color:#E1E4E8;">}</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span></code></pre></div><p>Then just run the calibration process by one line of code:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> logging</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">logging.basicConfig(</span></span>
<span class="line"><span style="color:#FFAB70;">    format</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;</span><span style="color:#79B8FF;">%(asctime)s</span><span style="color:#79B8FF;"> %(levelname)s</span><span style="color:#9ECBFF;"> [</span><span style="color:#79B8FF;">%(name)s</span><span style="color:#9ECBFF;">] </span><span style="color:#79B8FF;">%(message)s</span><span style="color:#9ECBFF;">&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">level</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">logging.</span><span style="color:#79B8FF;">INFO</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">datefmt</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;%Y-%m-</span><span style="color:#79B8FF;">%d</span><span style="color:#9ECBFF;"> %H:%M:%S&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model.quantize(data, </span><span style="color:#FFAB70;">cache_examples_on_gpu</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>Finally, save the quantized model:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">model.save_quantized(quant_path, </span><span style="color:#FFAB70;">use_safetensors</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer.save_pretrained(quant_path)</span></span></code></pre></div><p>It is unfortunate that the <code>save_quantized</code> method does not support sharding. For sharding, you need to load the model and use <code>save_pretrained</code> from transformers to save and shard the model. Except for this, everything is so simple. Enjoy!</p><h2 id="known-issues" tabindex="-1">Known Issues <a class="header-anchor" href="#known-issues" aria-label="Permalink to &quot;Known Issues&quot;">​</a></h2><h3 id="qwen2-5-72b-instruct-gptq-int4-cannot-stop-generation-properly" tabindex="-1">Qwen2.5-72B-Instruct-GPTQ-Int4 cannot stop generation properly <a class="header-anchor" href="#qwen2-5-72b-instruct-gptq-int4-cannot-stop-generation-properly" aria-label="Permalink to &quot;Qwen2.5-72B-Instruct-GPTQ-Int4 cannot stop generation properly&quot;">​</a></h3><p>:Model: Qwen2.5-72B-Instruct-GPTQ-Int4 :Framework: vLLM, AutoGPTQ (including Hugging Face transformers) :Description: Generation cannot stop properly. Continual generation after where it should stop, then repeated texts, either single character, a phrase, or paragraphs, are generated. :Workaround: The following workaround could be considered 1. Using the original model in 16-bit floating point 2. Using the AWQ variants or llama.cpp-based models for reduced chances of abnormal generation</p><h3 id="qwen2-5-32b-instruct-gptq-int4-broken-with-vllm-on-multiple-gpus" tabindex="-1">Qwen2.5-32B-Instruct-GPTQ-Int4 broken with vLLM on multiple GPUs <a class="header-anchor" href="#qwen2-5-32b-instruct-gptq-int4-broken-with-vllm-on-multiple-gpus" aria-label="Permalink to &quot;Qwen2.5-32B-Instruct-GPTQ-Int4 broken with vLLM on multiple GPUs&quot;">​</a></h3><p>:Model: Qwen2.5-32B-Instruct-GPTQ-Int4 :Framework: vLLM :Description: Deployment on multiple GPUs and only garbled text like <code>!!!!!!!!!!!!!!!!!!</code> could be generated. :Workaround: Each of the following workaround could be considered 1. Using the AWQ or GPTQ-Int8 variants 2. Using a single GPU 3. Using Hugging Face <code>transformers</code> if latency and throughput are not major concerns</p><h2 id="troubleshooting" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting" aria-label="Permalink to &quot;Troubleshooting&quot;">​</a></h2><p>:::{dropdown} With <code>transformers</code> and <code>auto_gptq</code>, the logs suggest <code>CUDA extension not installed.</code> and the inference is slow.</p><p><code>auto_gptq</code> fails to find a fused CUDA kernel compatible with your environment and falls back to a plain implementation. Follow its <a href="https://github.com/AutoGPTQ/AutoGPTQ/blob/main/docs/INSTALLATION.md" target="_blank" rel="noreferrer">installation guide</a> to install a pre-built wheel or try installing <code>auto_gptq</code> from source. :::</p><p>:::{dropdown} Self-quantized Qwen2.5-72B-Instruct-GPTQ with <code>vllm</code>, <code>ValueError: ... must be divisible by ...</code> is raised. The intermediate size of the self-quantized model is different from the official Qwen2.5-72B-Instruct-GPTQ models.</p><p>After quantization the size of the quantized weights are divided by the group size, which is typically 128. The intermediate size for the FFN blocks in Qwen2.5-72B is 29568. Unfortunately, {math}<code>29568 \\div 128 = 231</code>. Since the number of attention heads and the dimensions of the weights must be divisible by the tensor parallel size, it means you can only run the quantized model with <code>tensor_parallel_size=1</code>, i.e., one GPU card.</p><p>A workaround is to make the intermediate size divisible by {math}<code>128 \\times 8 = 1024</code>. To achieve that, the weights should be padded with zeros. While it is mathematically equivalent before and after zero-padding the weights, the results may be slightly different in reality.</p><p>Try the following:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch.nn </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> functional </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> F</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoModelForCausalLM</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># must use AutoModelForCausalLM</span></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span><span style="color:#9ECBFF;">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># this size is Qwen2.5-72B only</span></span>
<span class="line"><span style="color:#E1E4E8;">pad_size </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 128</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">sd </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.state_dict()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> i, k </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> enumerate</span><span style="color:#E1E4E8;">(sd):</span></span>
<span class="line"><span style="color:#E1E4E8;">    v </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> sd[k]</span></span>
<span class="line"><span style="color:#79B8FF;">    print</span><span style="color:#E1E4E8;">(k, i)</span></span>
<span class="line"><span style="color:#6A737D;">    # interleaving the padded zeros</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> (</span><span style="color:#9ECBFF;">&#39;mlp.up_proj.weight&#39;</span><span style="color:#F97583;"> in</span><span style="color:#E1E4E8;"> k) </span><span style="color:#F97583;">or</span><span style="color:#E1E4E8;"> (</span><span style="color:#9ECBFF;">&#39;mlp.gate_proj.weight&#39;</span><span style="color:#F97583;"> in</span><span style="color:#E1E4E8;"> k):</span></span>
<span class="line"><span style="color:#E1E4E8;">        prev_v </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> F.pad(v.unsqueeze(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">), (</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)).reshape(</span><span style="color:#79B8FF;">29568</span><span style="color:#F97583;">*</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)[:pad_size</span><span style="color:#F97583;">*</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">        new_v </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.cat([prev_v, v[pad_size:]], </span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">        sd[k] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> new_v</span></span>
<span class="line"><span style="color:#F97583;">    elif</span><span style="color:#9ECBFF;"> &#39;mlp.down_proj.weight&#39;</span><span style="color:#F97583;"> in</span><span style="color:#E1E4E8;"> k:</span></span>
<span class="line"><span style="color:#E1E4E8;">        prev_v</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> F.pad(v.unsqueeze(</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">), (</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)).reshape(</span><span style="color:#79B8FF;">8192</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">29568</span><span style="color:#F97583;">*</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)[:, :pad_size</span><span style="color:#F97583;">*</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">        new_v </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.cat([prev_v, v[:, pad_size:]], </span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">        sd[k] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> new_v</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># this is a very large file; make sure your RAM is enough to load the model</span></span>
<span class="line"><span style="color:#E1E4E8;">torch.save(sd, </span><span style="color:#9ECBFF;">&#39;/path/to/padded_model/pytorch_model.bin&#39;</span><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>This will save the padded checkpoint to the specified directory. Then, copy other files from the original checkpoint to the new directory and modify the <code>intermediate_size</code> in <code>config.json</code> to <code>29696</code>. Finally, you can quantize the saved model checkpoint. :::</p>`,45)]))}const F=a(p,[["render",t]]);export{d as __pageData,F as default};
