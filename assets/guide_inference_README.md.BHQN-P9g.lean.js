import{_ as n,c as a,b as e,o}from"./chunks/framework.B1z0IdBH.js";const d=JSON.parse('{"title":"Hugging Face transformers","description":"","frontmatter":{},"headers":[{"level":2,"title":"Basic Usage","slug":"basic-usage","link":"#basic-usage","children":[]},{"level":2,"title":"Batching","slug":"batching","link":"#batching","children":[]},{"level":2,"title":"Streaming Mode","slug":"streaming-mode","link":"#streaming-mode","children":[]},{"level":2,"title":"Using Flash Attention 2 to Accelerate Generation","slug":"using-flash-attention-2-to-accelerate-generation","link":"#using-flash-attention-2-to-accelerate-generation","children":[]},{"level":2,"title":"Troubleshooting","slug":"troubleshooting","link":"#troubleshooting","children":[]},{"level":2,"title":"Next Step","slug":"next-step","link":"#next-step","children":[]}],"relativePath":"guide/inference/README.md","filePath":"guide/inference/README.md"}'),p={name:"guide/inference/README.md"};function l(t,s,r,c,i,E){return o(),a("div",null,s[0]||(s[0]=[e(`<h1 id="hugging-face-transformers" tabindex="-1">Hugging Face transformers <a class="header-anchor" href="#hugging-face-transformers" aria-label="Permalink to &quot;Hugging Face transformers&quot;">​</a></h1><p>The most significant but also the simplest usage of Qwen2.5 is to chat with it using the <code>transformers</code> library. In this document, we show how to chat with <code>Qwen2.5-7B-Instruct</code>, in either streaming mode or not.</p><p>Select the interface you would like to use:</p><p>::::{tab-set} :sync-group: interface</p><p>:::{tab-item} Manual :sync: manual Using <code>AutoTokenizer</code> and <code>AutoModelForCausalLM</code>. :::</p><p>:::{tab-item} Pipeline :sync: pipeline Using <code>pipeline</code>. ::: ::::</p><h2 id="basic-usage" tabindex="-1">Basic Usage <a class="header-anchor" href="#basic-usage" aria-label="Permalink to &quot;Basic Usage&quot;">​</a></h2><p>::::{tab-set} :sync-group: interface</p><p>:::{tab-item} Manual :sync: manual</p><p>You can just write several lines of code with <code>transformers</code> to chat with Qwen2.5-Instruct. Essentially, we build the tokenizer and the model with <code>from_pretrained</code> method, and we use <code>generate</code> method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with Qwen2.5-7B-Instruct:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model_name </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Qwen/Qwen2.5-7B-Instruct&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_name,</span></span>
<span class="line"><span style="color:#FFAB70;">    torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(model_name)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">prompt </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Give me a short introduction to large language model.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">messages </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: prompt},</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color:#E1E4E8;">    messages,</span></span>
<span class="line"><span style="color:#FFAB70;">    tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model_inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer([text], </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">).to(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#F97583;">    **</span><span style="color:#E1E4E8;">model_inputs,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    output_ids[</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(input_ids):] </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> input_ids, output_ids </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(model_inputs.input_ids, generated_ids)</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.batch_decode(generated_ids, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span></code></pre></div><p>To continue the chat, simply append the response to the messages with the role assistant and repeat the procedure. The following shows and example:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">messages.append({</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;assistant&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: response})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">prompt </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Tell me more.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">messages.append({</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: prompt})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color:#E1E4E8;">    messages,</span></span>
<span class="line"><span style="color:#FFAB70;">    tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model_inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer([text], </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">).to(device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Directly use generate() and tokenizer.decode() to get the output.</span></span>
<span class="line"><span style="color:#6A737D;"># Use \`max_new_tokens\` to control the maximum output length.</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#F97583;">    **</span><span style="color:#E1E4E8;">model_inputs,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    output_ids[</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(input_ids):] </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> input_ids, output_ids </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(model_inputs.input_ids, generated_ids)</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.batch_decode(generated_ids, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span></code></pre></div><p>Note that the previous method in the original Qwen repo <code>chat()</code> is now replaced by <code>generate()</code>. The <code>apply_chat_template()</code> function is used to convert the messages into a format that the model can understand. The <code>add_generation_prompt</code> argument is used to add a generation prompt, which refers to <code>&lt;|im_start|&gt;assistant\\n</code> to the input. Notably, we apply ChatML template for chat models following our previous practice. The <code>max_new_tokens</code> argument is used to set the maximum length of the response. The <code>tokenizer.batch_decode()</code> function is used to decode the response. In terms of the input, the above <code>messages</code> is an example to show how to format your dialog history and system prompt. By default, if you do not specify system prompt, we directly use <code>You are Qwen, created by Alibaba Cloud. You are a helpful assistant.</code>. :::</p><p>:::{tab-item} Pipeline :sync: pipeline</p><p><code>transformers</code> provides a functionality called &quot;pipeline&quot; that encapsulates the many operations in common tasks. You can chat with the model in just 4 lines of code:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> pipeline</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">pipe </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipeline(</span><span style="color:#9ECBFF;">&quot;text-generation&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># the default system message will be used</span></span>
<span class="line"><span style="color:#E1E4E8;">messages </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [{</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Give me a short introduction to large language model.&quot;</span><span style="color:#E1E4E8;">}]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response_message </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipe(messages, </span><span style="color:#FFAB70;">max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">][</span><span style="color:#9ECBFF;">&quot;generated_text&quot;</span><span style="color:#E1E4E8;">][</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span></code></pre></div><p>To continue the chat, simply append the response to the messages with the role assistant and repeat the procedure. The following shows and example:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">messages.append(response_message)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">prompt </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Tell me more.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">messages.append({</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: prompt})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response_message </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipe(messages, </span><span style="color:#FFAB70;">max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">][</span><span style="color:#9ECBFF;">&quot;generated_text&quot;</span><span style="color:#E1E4E8;">][</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span></code></pre></div><p>::: ::::</p><h2 id="batching" tabindex="-1">Batching <a class="header-anchor" href="#batching" aria-label="Permalink to &quot;Batching&quot;">​</a></h2><p>:::{note} Batching is not automatically a win for performance. :::</p><p>All common <code>transformers</code> methods support batched input and output. For basic usage, the following is an example:</p><p>::::{tab-set} :sync-group: interface</p><p>:::{tab-item} Manual :sync: manual</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model_name </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Qwen/Qwen2.5-7B-Instruct&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_name,</span></span>
<span class="line"><span style="color:#FFAB70;">    torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(model_name, </span><span style="color:#FFAB70;">padding_side</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;left&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">message_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    [{</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Give me a detailed introduction to large language model.&quot;</span><span style="color:#E1E4E8;">}],</span></span>
<span class="line"><span style="color:#E1E4E8;">    [{</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Hello!&quot;</span><span style="color:#E1E4E8;">}],</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">text_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color:#E1E4E8;">    message_batch,</span></span>
<span class="line"><span style="color:#FFAB70;">    tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model_inputs_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer(text_batch, </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">padding</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">).to(model.device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#F97583;">    **</span><span style="color:#E1E4E8;">model_inputs_batch,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> generated_ids_batch[:, model_inputs_batch.input_ids.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]:]</span></span>
<span class="line"><span style="color:#E1E4E8;">response_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.batch_decode(generated_ids_batch, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>:::</p><p>:::{tab-item} Pipeline :sync: pipeline</p><p>With pipeline, it is simpler:</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> pipeline</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">pipe </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipeline(</span><span style="color:#9ECBFF;">&quot;text-generation&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">pipe.tokenizer.padding_side</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;left&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">message_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    [{</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Give me a detailed introduction to large language model.&quot;</span><span style="color:#E1E4E8;">}],</span></span>
<span class="line"><span style="color:#E1E4E8;">    [{</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Hello!&quot;</span><span style="color:#E1E4E8;">}],</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">result_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipe(message_batch, </span><span style="color:#FFAB70;">max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">batch_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">response_message_batch </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [result[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">][</span><span style="color:#9ECBFF;">&quot;generated_text&quot;</span><span style="color:#E1E4E8;">][</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> result </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> result_batch]</span></span></code></pre></div><p>::: ::::</p><h2 id="streaming-mode" tabindex="-1">Streaming Mode <a class="header-anchor" href="#streaming-mode" aria-label="Permalink to &quot;Streaming Mode&quot;">​</a></h2><p>With the help of <code>TextStreamer</code>, you can modify your chatting with Qwen to streaming mode. It will print the response as being generated to the console or the terminal. Below we show you an example of how to use it:</p><p>::::{tab-set} :sync-group: interface</p><p>:::{tab-item} Manual :sync: manual</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># Repeat the code above before model.generate()</span></span>
<span class="line"><span style="color:#6A737D;"># Starting here, we add streamer for text generation.</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> TextStreamer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">streamer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> TextStreamer(tokenizer, </span><span style="color:#FFAB70;">skip_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_inputs,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    streamer</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">streamer,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>:::</p><p>:::{tab-item} Pipeline :sync: pipeline</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> pipeline, TextStreamer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">pipe </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipeline(</span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;text-generation&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#FFAB70;">    torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">streamer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> TextStreamer(pipe.tokenizer, </span><span style="color:#FFAB70;">skip_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response_message </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipe(messages, </span><span style="color:#FFAB70;">max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">streamer</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">streamer)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">][</span><span style="color:#9ECBFF;">&quot;generated_text&quot;</span><span style="color:#E1E4E8;">][</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span></code></pre></div><p>::: ::::</p><p>Besides using <code>TextStreamer</code>, we can also use <code>TextIteratorStreamer</code> which stores print-ready text in a queue, to be used by a downstream application as an iterator:</p><p>::::{tab-set} :sync-group: interface</p><p>:::{tab-item} Manual :sync: manual</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># Repeat the code above before model.generate()</span></span>
<span class="line"><span style="color:#6A737D;"># Starting here, we add streamer for text generation.</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> TextIteratorStreamer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">streamer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> TextIteratorStreamer(tokenizer, </span><span style="color:#FFAB70;">skip_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Use Thread to run generation in background</span></span>
<span class="line"><span style="color:#6A737D;"># Otherwise, the process is blocked until generation is complete</span></span>
<span class="line"><span style="color:#6A737D;"># and no streaming effect can be observed.</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> threading </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> Thread</span></span>
<span class="line"><span style="color:#E1E4E8;">generation_kwargs </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> dict</span><span style="color:#E1E4E8;">(model_inputs, </span><span style="color:#FFAB70;">streamer</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">streamer, </span><span style="color:#FFAB70;">max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">thread </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Thread(</span><span style="color:#FFAB70;">target</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">model.generate, </span><span style="color:#FFAB70;">kwargs</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">generation_kwargs)</span></span>
<span class="line"><span style="color:#E1E4E8;">thread.start()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_text </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;&quot;</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> new_text </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> streamer:</span></span>
<span class="line"><span style="color:#E1E4E8;">    generated_text </span><span style="color:#F97583;">+=</span><span style="color:#E1E4E8;"> new_text</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(generated_text)</span></span></code></pre></div><p>:::</p><p>:::{tab-item} Pipeline :sync: pipeline</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> pipeline, TextIteratorStreamer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">pipe </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipeline(</span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;text-generation&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#FFAB70;">    torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">streamer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> TextIteratorStreamer(pipe.tokenizer, </span><span style="color:#FFAB70;">skip_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Use Thread to run generation in background</span></span>
<span class="line"><span style="color:#6A737D;"># Otherwise, the process is blocked until generation is complete</span></span>
<span class="line"><span style="color:#6A737D;"># and no streaming effect can be observed.</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> threading </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> Thread</span></span>
<span class="line"><span style="color:#E1E4E8;">generation_kwargs </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> dict</span><span style="color:#E1E4E8;">(</span><span style="color:#FFAB70;">text_inputs</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">messages, </span><span style="color:#FFAB70;">max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">streamer</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">streamer)</span></span>
<span class="line"><span style="color:#E1E4E8;">thread </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Thread(</span><span style="color:#FFAB70;">target</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">pipe, </span><span style="color:#FFAB70;">kwargs</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">generation_kwargs)</span></span>
<span class="line"><span style="color:#E1E4E8;">thread.start()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_text </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;&quot;</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> new_text </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> streamer:</span></span>
<span class="line"><span style="color:#E1E4E8;">    generated_text </span><span style="color:#F97583;">+=</span><span style="color:#E1E4E8;"> new_text</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(generated_text)</span></span></code></pre></div><p>::: ::::</p><h2 id="using-flash-attention-2-to-accelerate-generation" tabindex="-1">Using Flash Attention 2 to Accelerate Generation <a class="header-anchor" href="#using-flash-attention-2-to-accelerate-generation" aria-label="Permalink to &quot;Using Flash Attention 2 to Accelerate Generation&quot;">​</a></h2><p>:::{note} With the latest <code>transformers</code> and <code>torch</code>, Flash Attention 2 will be applied by default if applicable.[^fa2] You do not need to request the use of Flash Attention 2 in <code>transformers</code> or install the <code>flash_attn</code> package. The following is intended for users that cannot use the latest versions for various reasons. :::</p><p>If you would like to apply Flash Attention 2, you need to install an appropriate version of <code>flash_attn</code>. You can find pre-built wheels at <a href="https://github.com/Dao-AILab/flash-attention/releases" target="_blank" rel="noreferrer">its GitHub repository</a>, and you should make sure the Python version, the torch version, and the CUDA version of torch are a match. Otherwise, you need to install from source. Please follow the guides at <a href="https://github.com/Dao-AILab/flash-attention" target="_blank" rel="noreferrer">its GitHub README</a>.</p><p>After a successful installation, you can load the model as shown below:</p><p>::::{tab-set} :sync-group: interface</p><p>:::{tab-item} Manual :sync: manual</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="color:#9ECBFF;">   &quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">   torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">   device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">   attn_implementation</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;flash_attention_2&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>::: :::{tab-item} Pipeline :sync: pipeline</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">pipe </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pipeline(</span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;text-generation&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#FFAB70;">    torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">, </span></span>
<span class="line"><span style="color:#FFAB70;">    model_kwargs</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">dict</span><span style="color:#E1E4E8;">(</span><span style="color:#FFAB70;">attn_implementation</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;flash_attention_2&quot;</span><span style="color:#E1E4E8;">),</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span></code></pre></div><p>::: ::::</p><p>[^fa2]: The attention module for a model in <code>transformers</code> typically has three variants: <code>sdpa</code>, <code>flash_attention_2</code>, and <code>eager</code>. The first two are wrappers around related functions in the <code>torch</code> and the <code>flash_attn</code> packages. It defaults to <code>sdpa</code> if available.</p><pre><code>   In addition, \`torch\` has integrated three implementations for \`sdpa\`: \`FLASH_ATTENTION\` (indicating Flash Attention 2 since version 2.2), \`EFFICIENT_ATTENTION\` (Memory Efficient Attention), and \`MATH\`.
   It attempts to automatically select the most optimal implementation based on the inputs.
   You don&#39;t need to install extra packages to use them.

   Hence, if applicable, by default, \`transformers\` uses \`sdpa\` and \`torch\` selects \`FLASH_ATTENTION\`.

   If you wish to explicitly select the implementations in \`torch\`, refer to [this tutorial](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html).
</code></pre><h2 id="troubleshooting" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting" aria-label="Permalink to &quot;Troubleshooting&quot;">​</a></h2><p>:::{dropdown} Loading models takes a lot of memory</p><p>Normally, memory usage after loading the model can be roughly taken as twice the parameter count. For example, a 7B model will take 14GB memory to load. It is because for large language models, the compute dtype is often 16-bit floating point number. Of course, you will need more memory in inference to store the activations.</p><p>For <code>transformers</code>, <code>torch_dtype=&quot;auto&quot;</code> is recommended and the model will be loaded in <code>bfloat16</code> automatically. Otherwise, the model will be loaded in <code>float32</code> and it will need double memory. You can also pass <code>torch.bfloat16</code> or <code>torch.float16</code> as <code>torch_dtype</code> explicitly. :::</p><p>:::{dropdown} Multi-GPU inference is slow</p><p><code>transformers</code> relies on <code>accelerate</code> for multi-GPU inference and the implementation is a kind of naive model parallelism: different GPUs computes different layers of the model. It is enabled by the use of <code>device_map=&quot;auto&quot;</code> or a customized <code>device_map</code> for multiple GPUs.</p><p>However, this kind of implementation is not efficient as for a single request, only one GPU computes at the same time and the other GPUs just wait. To use all the GPUs, you need to arrange multiple sequences as on a pipeline, making sure each GPU has some work to do. However, that will require concurrency management and load balancing, which is out of the scope of <code>transformers</code>. Even if all things are implemented, you can make use of concurrency to improve the total throughput but the latency for each request is not great.</p><p>For Multi-GPU inference, we recommend using specialized inference framework, such as vLLM and TGI, which support tensor parallelism. :::</p><p>:::{dropdown} <code>RuntimeError: CUDA error: device-side assert triggered</code>, <code>Assertion -sizes[i] &lt;= index &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot; failed.</code></p><p>If it works with single GPU but not multiple GPUs, especially if there are PCI-E switches in your system, it could be related to drivers.</p><ol><li><p>Try upgrading the GPU driver.</p><p>For data center GPUs (e.g., A800, H800, and L40s), please use the data center GPU drivers and upgrade to the latest subrelease, e.g., 535.104.05 to 535.183.01. You can check the release note at <a href="https://docs.nvidia.com/datacenter/tesla/index.html" target="_blank" rel="noreferrer">https://docs.nvidia.com/datacenter/tesla/index.html</a>, where the issues fixed and known issues are presented.</p><p>For consumer GPUs (e.g., RTX 3090 and RTX 4090), their GPU drivers are released more frequently and focus more on gaming optimization. There are online reports that 545.29.02 breaks <code>vllm</code> and <code>torch</code> but 545.29.06 works. Their release notes are also less helpful in identifying the real issues. However, in general, the advice is still upgrading the GPU driver.</p></li><li><p>Try disabling P2P for process hang, but it has negative effect on speed.</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>export NCCL_P2P_DISABLE=1</span></span></code></pre></div></li></ol><p>:::</p><h2 id="next-step" tabindex="-1">Next Step <a class="header-anchor" href="#next-step" aria-label="Permalink to &quot;Next Step&quot;">​</a></h2><p>Now you can chat with Qwen2.5 in either streaming mode or not. Continue to read the documentation and try to figure out more advanced usages of model inference!</p>`,74)]))}const u=n(p,[["render",l]]);export{d as __pageData,u as default};
