import{_ as a,c as n,b as l,o as t}from"./chunks/framework.B1z0IdBH.js";const y=JSON.parse('{"title":"使用LLaMA-Factory微调Qwen模型","description":"","frontmatter":{},"headers":[{"level":2,"title":"LLAMA-Factory简介","slug":"llama-factory简介","link":"#llama-factory简介","children":[]},{"level":2,"title":"安装LLaMA-Factory","slug":"安装llama-factory","link":"#安装llama-factory","children":[]},{"level":2,"title":"准备训练数据","slug":"准备训练数据","link":"#准备训练数据","children":[]},{"level":2,"title":"配置训练参数","slug":"配置训练参数","link":"#配置训练参数","children":[]},{"level":2,"title":"开始训练","slug":"开始训练","link":"#开始训练","children":[]},{"level":2,"title":"合并模型权重","slug":"合并模型权重","link":"#合并模型权重","children":[]},{"level":2,"title":"模型推理","slug":"模型推理","link":"#模型推理","children":[]}],"relativePath":"guide/llama-factory/README.md","filePath":"guide/llama-factory/README.md"}'),o={name:"guide/llama-factory/README.md"};function e(p,s,r,c,i,d){return t(),n("div",null,s[0]||(s[0]=[l(`<h1 id="使用llama-factory微调qwen模型" tabindex="-1">使用LLaMA-Factory微调Qwen模型 <a class="header-anchor" href="#使用llama-factory微调qwen模型" aria-label="Permalink to &quot;使用LLaMA-Factory微调Qwen模型&quot;">​</a></h1><h2 id="llama-factory简介" tabindex="-1">LLAMA-Factory简介 <a class="header-anchor" href="#llama-factory简介" aria-label="Permalink to &quot;LLAMA-Factory简介&quot;">​</a></h2><p><a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noreferrer">LLaMA-Factory</a>是一个简单易用且高效的大模型训练框架，支持上百种大模型的训练，框架特性主要包括：</p><ul><li>模型种类：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。</li><li>训练算法：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。</li><li>运算精度：16比特全参数微调、冻结微调、LoRA微调和基于AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ的2/3/4/5/6/8比特QLoRA 微调。</li><li>优化算法：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ和PiSSA。</li><li>加速算子：FlashAttention-2和Unsloth。</li><li>推理引擎：Transformers和vLLM。</li><li>实验面板：LlamaBoard、TensorBoard、Wandb、MLflow等等。</li></ul><p>本文将介绍如何使用LLAMA-Factory对Qwen2系列大模型进行微调（Qwen1.5系列模型也适用），更多特性请参考<a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noreferrer">LLaMA-Factory</a>。</p><h2 id="安装llama-factory" tabindex="-1">安装LLaMA-Factory <a class="header-anchor" href="#安装llama-factory" aria-label="Permalink to &quot;安装LLaMA-Factory&quot;">​</a></h2><p>下载并安装LLaMA-Factory：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">git</span><span style="color:#9ECBFF;"> clone</span><span style="color:#79B8FF;"> --depth</span><span style="color:#79B8FF;"> 1</span><span style="color:#9ECBFF;"> https://github.com/hiyouga/LLaMA-Factory.git</span></span>
<span class="line"><span style="color:#79B8FF;">cd</span><span style="color:#9ECBFF;"> LLaMA-Factory</span></span>
<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#79B8FF;"> -e</span><span style="color:#9ECBFF;"> &quot;.[torch,metrics]&quot;</span></span></code></pre></div><p>安装完成后，执行<code>llamafactory-cli version</code>，若出现以下提示，则表明安装成功：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>----------------------------------------------------------</span></span>
<span class="line"><span>| Welcome to LLaMA Factory, version 0.8.4.dev0           |</span></span>
<span class="line"><span>|                                                        |</span></span>
<span class="line"><span>| Project page: https://github.com/hiyouga/LLaMA-Factory |</span></span>
<span class="line"><span>----------------------------------------------------------</span></span></code></pre></div><h2 id="准备训练数据" tabindex="-1">准备训练数据 <a class="header-anchor" href="#准备训练数据" aria-label="Permalink to &quot;准备训练数据&quot;">​</a></h2><p>自定义的训练数据应保存为jsonl文件，每一行的格式如下：</p><div class="language-json"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">{</span></span>
<span class="line"><span style="color:#79B8FF;">    &quot;messages&quot;</span><span style="color:#E1E4E8;">: [</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are a helpful assistant.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        },</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Tell me something about large language models.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        },</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;assistant&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Large language models are a type of language model that is trained on a large corpus of text data. They are capable of generating human-like text and are used in a variety of natural language processing tasks...&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        },</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;How about Qwen2?&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        },</span></span>
<span class="line"><span style="color:#E1E4E8;">        {</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;assistant&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#79B8FF;">            &quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;Qwen2 is a large language model developed by Alibaba Cloud...&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">        }</span></span>
<span class="line"><span style="color:#E1E4E8;">      </span></span>
<span class="line"><span style="color:#E1E4E8;">    ]</span></span>
<span class="line"><span style="color:#E1E4E8;">}</span></span></code></pre></div><p>在LLaMA-Factory文件夹下的<code>data/dataset_info.json</code>文件中注册自定义的训练数据，在文件尾部添加如下配置信息：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>&quot;qwen_train_data&quot;: {</span></span>
<span class="line"><span>    &quot;file_name&quot;: &quot;PATH-TO-YOUR-TRAIN-DATA&quot;,</span></span>
<span class="line"><span>    &quot;formatting&quot;: &quot;sharegpt&quot;,</span></span>
<span class="line"><span>    &quot;columns&quot;: {</span></span>
<span class="line"><span>      &quot;messages&quot;: &quot;messages&quot;</span></span>
<span class="line"><span>    },</span></span>
<span class="line"><span>    &quot;tags&quot;: {</span></span>
<span class="line"><span>      &quot;role_tag&quot;: &quot;role&quot;,</span></span>
<span class="line"><span>      &quot;content_tag&quot;: &quot;content&quot;,</span></span>
<span class="line"><span>      &quot;user_tag&quot;: &quot;user&quot;,</span></span>
<span class="line"><span>      &quot;assistant_tag&quot;: &quot;assistant&quot;,</span></span>
<span class="line"><span>      &quot;system_tag&quot;: &quot;system&quot;</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>}</span></span></code></pre></div><h2 id="配置训练参数" tabindex="-1">配置训练参数 <a class="header-anchor" href="#配置训练参数" aria-label="Permalink to &quot;配置训练参数&quot;">​</a></h2><p>设置训练参数的配置文件，我们提供了全量参数、LoRA、QLoRA训练所对应的示例文件，你可以根据自身需求自行修改，配置详情见本目录下对应的文件:</p><ul><li><code>qwen2-7b-full-sft.yaml</code>: 全量参数训练</li><li><code>qwen2-7b-lora-sft.yaml</code>: LoRA训练</li><li><code>qwen2-7b-qlora-sft.yaml</code>: QLoRA训练</li></ul><p>全量参数训练时的deepspeed配置文件可参考<a href="https://github.com/hiyouga/LLaMA-Factory/tree/main/examples/deepspeed" target="_blank" rel="noreferrer">文件</a></p><p>部分训练参数说明：</p><table tabindex="0"><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>model_name_or_path</td><td>模型名称或路径</td></tr><tr><td>stage</td><td>训练阶段，可选: rm(reward modeling), pt(pretrain), sft(Supervised Fine-Tuning), PPO, DPO, KTO, ORPO</td></tr><tr><td>do_train</td><td>true用于训练, false用于评估</td></tr><tr><td>finetuning_type</td><td>微调方式。可选: freeze, LoRA, full</td></tr><tr><td>lora_target</td><td>采取LoRA方法的目标模块，默认值为all。</td></tr><tr><td>dataset</td><td>使用的数据集，使用”,”分隔多个数据集</td></tr><tr><td>template</td><td>数据集模板，请保证数据集模板与模型相对应。</td></tr><tr><td>output_dir</td><td>输出路径</td></tr><tr><td>logging_steps</td><td>日志输出步数间隔</td></tr><tr><td>save_steps</td><td>模型断点保存间隔</td></tr><tr><td>overwrite_output_dir</td><td>是否允许覆盖输出目录</td></tr><tr><td>per_device_train_batch_size</td><td>每个设备上训练的批次大小</td></tr><tr><td>gradient_accumulation_steps</td><td>梯度积累步数</td></tr><tr><td>learning_rate</td><td>学习率</td></tr><tr><td>lr_scheduler_type</td><td>学习率曲线，可选 linear, cosine, polynomial, constant 等。</td></tr><tr><td>num_train_epochs</td><td>训练周期数</td></tr><tr><td>bf16</td><td>是否使用 bf16 格式</td></tr></tbody></table><h2 id="开始训练" tabindex="-1">开始训练 <a class="header-anchor" href="#开始训练" aria-label="Permalink to &quot;开始训练&quot;">​</a></h2><p>全量参数训练：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">FORCE_TORCHRUN</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">1</span><span style="color:#B392F0;"> llamafactory-cli</span><span style="color:#9ECBFF;"> train</span><span style="color:#9ECBFF;"> qwen2-7b-full-sft.yaml</span></span></code></pre></div><p>LoRA训练：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">llamafactory-cli</span><span style="color:#9ECBFF;"> train</span><span style="color:#9ECBFF;"> qwen2-7b-lora-sft.yaml</span></span></code></pre></div><p>QLoRA训练：</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">llamafactory-cli</span><span style="color:#9ECBFF;"> train</span><span style="color:#9ECBFF;"> qwen2-7b-qlora-sft.yaml</span></span></code></pre></div><p>使用上述训练配置，各个方法实测的显存占用如下。训练中的显存占用与训练参数配置息息相关，可根据自身实际需求进行设置。</p><ul><li>全量参数训练：42.18GB</li><li>LoRA训练：20.17GB</li><li>QLoRA训练: 10.97GB</li></ul><h2 id="合并模型权重" tabindex="-1">合并模型权重 <a class="header-anchor" href="#合并模型权重" aria-label="Permalink to &quot;合并模型权重&quot;">​</a></h2><p>如果采用LoRA或者QLoRA进行训练，脚本只保存对应的LoRA权重，需要合并权重才能进行推理。<strong>全量参数训练无需执行此步骤</strong></p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">llamafactory-cli</span><span style="color:#9ECBFF;"> export</span><span style="color:#9ECBFF;"> qwen2-7b-merge-lora.yaml</span></span></code></pre></div><p>权重合并的部分参数说明：</p><table tabindex="0"><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>model_name_or_path</td><td>预训练模型的名称或路径</td></tr><tr><td>template</td><td>模型模板</td></tr><tr><td>export_dir</td><td>导出路径</td></tr><tr><td>export_size</td><td>最大导出模型文件大小</td></tr><tr><td>export_device</td><td>导出设备</td></tr><tr><td>export_legacy_format</td><td>是否使用旧格式导出</td></tr></tbody></table><p>注意：</p><ul><li>合并Qwen2模型权重，务必将template设为<code>qwen</code>；无论LoRA还是QLoRA训练，合并权重时，<code>finetuning_type</code>均为<code>lora</code>。</li><li>adapter_name_or_path需要与微调中的适配器输出路径output_dir相对应。</li></ul><h2 id="模型推理" tabindex="-1">模型推理 <a class="header-anchor" href="#模型推理" aria-label="Permalink to &quot;模型推理&quot;">​</a></h2><p>训练完成，合并模型权重之后，即可加载完整的模型权重进行推理， 推理的示例脚本如下：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"><span style="color:#E1E4E8;">device </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;cuda&quot;</span><span style="color:#6A737D;"> # the device to load the model onto</span></span>
<span class="line"><span style="color:#E1E4E8;">model_name_or_path </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> YOUR</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">MODEL</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">PATH</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_name_or_path,</span></span>
<span class="line"><span style="color:#FFAB70;">    torch_dtype</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    device_map</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;auto&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(model_name_or_path)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">prompt </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Give me a short introduction to large language model.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">messages </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: prompt}</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color:#E1E4E8;">    messages,</span></span>
<span class="line"><span style="color:#FFAB70;">    tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model_inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer([text], </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">).to(device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model.generate(</span></span>
<span class="line"><span style="color:#E1E4E8;">    model_inputs.input_ids,</span></span>
<span class="line"><span style="color:#FFAB70;">    max_new_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">generated_ids </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    output_ids[</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(input_ids):] </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> input_ids, output_ids </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(model_inputs.input_ids, generated_ids)</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.batch_decode(generated_ids, </span><span style="color:#FFAB70;">skip_special_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span></code></pre></div>`,40)]))}const u=a(o,[["render",e]]);export{y as __pageData,u as default};
