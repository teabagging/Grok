import{_ as s,c as n,b as a,o as l}from"./chunks/framework.B1z0IdBH.js";const u=JSON.parse('{"title":"Speed Benchmark","description":"","frontmatter":{},"headers":[{"level":2,"title":"1. Model Collections","slug":"_1-model-collections","link":"#_1-model-collections","children":[]},{"level":2,"title":"2. Environment Setup","slug":"_2-environment-setup","link":"#_2-environment-setup","children":[]},{"level":2,"title":"3. Execute Tests","slug":"_3-execute-tests","link":"#_3-execute-tests","children":[{"level":3,"title":"Method 1: Testing with Speed Benchmark Tool","slug":"method-1-testing-with-speed-benchmark-tool","link":"#method-1-testing-with-speed-benchmark-tool","children":[]},{"level":3,"title":"Method 2: Testing with Scripts","slug":"method-2-testing-with-scripts","link":"#method-2-testing-with-scripts","children":[]}]},{"level":2,"title":"Notes","slug":"notes","link":"#notes","children":[]}],"relativePath":"guide/speed-benchmark/README.md","filePath":"guide/speed-benchmark/README.md"}'),o={name:"guide/speed-benchmark/README.md"};function t(p,e,r,c,i,d){return l(),n("div",null,e[0]||(e[0]=[a('<h1 id="speed-benchmark" tabindex="-1">Speed Benchmark <a class="header-anchor" href="#speed-benchmark" aria-label="Permalink to &quot;Speed Benchmark&quot;">â€‹</a></h1><p>This document introduces the speed benchmark testing process for the Qwen2.5 series models (original and quantized models). For detailed reports, please refer to the <a href="https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html" target="_blank" rel="noreferrer">Qwen2.5 Speed Benchmark</a>.</p><h2 id="_1-model-collections" tabindex="-1">1. Model Collections <a class="header-anchor" href="#_1-model-collections" aria-label="Permalink to &quot;1. Model Collections&quot;">â€‹</a></h2><p>For models hosted on HuggingFace, refer to <a href="https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e" target="_blank" rel="noreferrer">Qwen2.5 Collections-HuggingFace</a>.</p><p>For models hosted on ModelScope, refer to <a href="https://modelscope.cn/collections/Qwen25-dbc4d30adb768" target="_blank" rel="noreferrer">Qwen2.5 Collections-ModelScope</a>.</p><h2 id="_2-environment-setup" tabindex="-1">2. Environment Setup <a class="header-anchor" href="#_2-environment-setup" aria-label="Permalink to &quot;2. Environment Setup&quot;">â€‹</a></h2><p>For inference using HuggingFace transformers:</p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> create</span><span style="color:#79B8FF;"> -n</span><span style="color:#9ECBFF;"> qwen_perf_transformers</span><span style="color:#9ECBFF;"> python=</span><span style="color:#79B8FF;">3.10</span></span>\n<span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> activate</span><span style="color:#9ECBFF;"> qwen_perf_transformers</span></span>\n<span class="line"></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> torch==</span><span style="color:#79B8FF;">2.3.1</span></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> git+https://github.com/AutoGPTQ/AutoGPTQ.git@v0.7.1</span></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> git+https://github.com/Dao-AILab/flash-attention.git@v2.5.8</span></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#79B8FF;"> -r</span><span style="color:#9ECBFF;"> requirements-perf-transformers.txt</span></span></code></pre></div><div class="important custom-block github-alert"><p class="custom-block-title">IMPORTANT</p><p></p><ul><li>For <code>flash-attention</code>, you can use the prebulit wheels from <a href="https://github.com/Dao-AILab/flash-attention/releases/tag/v2.5.8" target="_blank" rel="noreferrer">GitHub Releases</a> or installing from source, which requires a compatible CUDA compiler. <ul><li>You don&#39;t actually need to install <code>flash-attention</code>. It has been intergrated into <code>torch</code> as a backend of <code>sdpa</code>.</li></ul></li><li>For <code>auto_gptq</code> to use efficent kernels, you need to install from source, because the prebuilt wheels require incompatible <code>torch</code> versions. Installing from source also requires a compatible CUDA compiler.</li><li>For <code>autoawq</code> to use efficent kenerls, you need <code>autoawq-kernels</code>, which should be automatically installed. If not, run <code>pip install autoawq-kernels</code>.</li></ul></div><p>For inference using vLLM:</p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> create</span><span style="color:#79B8FF;"> -n</span><span style="color:#9ECBFF;"> qwen_perf_vllm</span><span style="color:#9ECBFF;"> python=</span><span style="color:#79B8FF;">3.10</span></span>\n<span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> activate</span><span style="color:#9ECBFF;"> qwen_perf_vllm</span></span>\n<span class="line"></span>\n<span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#79B8FF;"> -r</span><span style="color:#9ECBFF;"> requirements-perf-vllm.txt</span></span></code></pre></div><h2 id="_3-execute-tests" tabindex="-1">3. Execute Tests <a class="header-anchor" href="#_3-execute-tests" aria-label="Permalink to &quot;3. Execute Tests&quot;">â€‹</a></h2><p>Below are two methods for executing tests: using a script or the Speed Benchmark tool.</p><h3 id="method-1-testing-with-speed-benchmark-tool" tabindex="-1">Method 1: Testing with Speed Benchmark Tool <a class="header-anchor" href="#method-1-testing-with-speed-benchmark-tool" aria-label="Permalink to &quot;Method 1: Testing with Speed Benchmark Tool&quot;">â€‹</a></h3><p>Use the Speed Benchmark tool developed by <a href="https://github.com/modelscope/evalscope" target="_blank" rel="noreferrer">EvalScope</a>, which supports automatic model downloads from ModelScope and outputs test results. It also allows testing by specifying the model service URL. For details, please refer to the <a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/speed_benchmark.html" target="_blank" rel="noreferrer">ðŸ“– User Guide</a>.</p><p><strong>Install Dependencies</strong></p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> &#39;evalscope[perf]&#39;</span><span style="color:#79B8FF;"> -U</span></span></code></pre></div><h4 id="huggingface-transformers-inference" tabindex="-1">HuggingFace Transformers Inference <a class="header-anchor" href="#huggingface-transformers-inference" aria-label="Permalink to &quot;HuggingFace Transformers Inference&quot;">â€‹</a></h4><p>Execute the command as follows:</p><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">CUDA_VISIBLE_DEVICES</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">0</span><span style="color:#B392F0;"> evalscope</span><span style="color:#9ECBFF;"> perf</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --parallel</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --model</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --attn-implementation</span><span style="color:#9ECBFF;"> flash_attention_2</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --log-every-n-query</span><span style="color:#79B8FF;"> 5</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --connect-timeout</span><span style="color:#79B8FF;"> 6000</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --read-timeout</span><span style="color:#79B8FF;"> 6000</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --max-tokens</span><span style="color:#79B8FF;"> 2048</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --min-tokens</span><span style="color:#79B8FF;"> 2048</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --api</span><span style="color:#9ECBFF;"> local</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --dataset</span><span style="color:#9ECBFF;"> speed_benchmark</span></span></code></pre></div><h4 id="vllm-inference" tabindex="-1">vLLM Inference <a class="header-anchor" href="#vllm-inference" aria-label="Permalink to &quot;vLLM Inference&quot;">â€‹</a></h4><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">CUDA_VISIBLE_DEVICES</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">0</span><span style="color:#B392F0;"> evalscope</span><span style="color:#9ECBFF;"> perf</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --parallel</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --model</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --log-every-n-query</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --connect-timeout</span><span style="color:#79B8FF;"> 60000</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --read-timeout</span><span style="color:#79B8FF;"> 60000</span><span style="color:#79B8FF;">\\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --max-tokens</span><span style="color:#79B8FF;"> 2048</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --min-tokens</span><span style="color:#79B8FF;"> 2048</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --api</span><span style="color:#9ECBFF;"> local_vllm</span><span style="color:#79B8FF;"> \\</span></span>\n<span class="line"><span style="color:#79B8FF;"> --dataset</span><span style="color:#9ECBFF;"> speed_benchmark</span></span></code></pre></div><h4 id="parameter-explanation" tabindex="-1">Parameter Explanation <a class="header-anchor" href="#parameter-explanation" aria-label="Permalink to &quot;Parameter Explanation&quot;">â€‹</a></h4><ul><li><code>--parallel</code> sets the number of worker threads for concurrent requests, should be fixed at 1.</li><li><code>--model</code> specifies the model file path or model ID, supporting automatic downloads from ModelScope, e.g., Qwen/Qwen2.5-0.5B-Instruct.</li><li><code>--attn-implementation</code> sets the attention implementation method, with optional values: flash_attention_2|eager|sdpa.</li><li><code>--log-every-n-query</code>: sets how often to log every n requests.</li><li><code>--connect-timeout</code>: sets the connection timeout in seconds.</li><li><code>--read-timeout</code>: sets the read timeout in seconds.</li><li><code>--max-tokens</code>: sets the maximum output length in tokens.</li><li><code>--min-tokens</code>: sets the minimum output length in tokens; both parameters set to 2048 means the model will output a fixed length of 2048.</li><li><code>--api</code>: sets the inference interface; local inference options are local|local_vllm.</li><li><code>--dataset</code>: sets the test dataset; options are speed_benchmark|speed_benchmark_long.</li></ul><h4 id="test-results" tabindex="-1">Test Results <a class="header-anchor" href="#test-results" aria-label="Permalink to &quot;Test Results&quot;">â€‹</a></h4><p>Test results can be found in the <code>outputs/{model_name}/{timestamp}/speed_benchmark.json</code> file, which contains all request results and test parameters.</p><h3 id="method-2-testing-with-scripts" tabindex="-1">Method 2: Testing with Scripts <a class="header-anchor" href="#method-2-testing-with-scripts" aria-label="Permalink to &quot;Method 2: Testing with Scripts&quot;">â€‹</a></h3><h4 id="huggingface-transformers-inference-1" tabindex="-1">HuggingFace Transformers Inference <a class="header-anchor" href="#huggingface-transformers-inference-1" aria-label="Permalink to &quot;HuggingFace Transformers Inference&quot;">â€‹</a></h4><ul><li>Using HuggingFace Hub</li></ul><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#9ECBFF;"> speed_benchmark_transformers.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/transformers</span></span></code></pre></div><ul><li>Using ModelScope Hub</li></ul><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#9ECBFF;"> speed_benchmark_transformers.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --use_modelscope</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/transformers</span></span></code></pre></div><p>Parameter Explanation:</p><pre><code>`--model_id_or_path`: Model ID or local path, optional values refer to the `Model Resources` section  \n`--context_length`: Input length in tokens; optional values are 1, 6144, 14336, 30720, 63488, 129024; refer to the `Qwen2.5 Model Efficiency Evaluation Report` for specifics  \n`--generate_length`: Number of tokens to generate; default is 2048\n`--gpus`: Equivalent to the environment variable CUDA_VISIBLE_DEVICES, e.g., `0,1,2,3`, `4,5`  \n`--use_modelscope`: If set, uses ModelScope to load the model; otherwise, uses HuggingFace  \n`--outputs_dir`: Output directory, default is `outputs/transformers`  \n</code></pre><h4 id="vllm-inference-1" tabindex="-1">vLLM Inference <a class="header-anchor" href="#vllm-inference-1" aria-label="Permalink to &quot;vLLM Inference&quot;">â€‹</a></h4><ul><li>Using HuggingFace Hub</li></ul><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#9ECBFF;"> speed_benchmark_vllm.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --max_model_len</span><span style="color:#79B8FF;"> 32768</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --gpu_memory_utilization</span><span style="color:#79B8FF;"> 0.9</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/vllm</span></span></code></pre></div><ul><li>Using ModelScope Hub</li></ul><div class="language-shell"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">python</span><span style="color:#9ECBFF;"> speed_benchmark_vllm.py</span><span style="color:#79B8FF;"> --model_id_or_path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-0.5B-Instruct</span><span style="color:#79B8FF;"> --context_length</span><span style="color:#79B8FF;"> 1</span><span style="color:#79B8FF;"> --max_model_len</span><span style="color:#79B8FF;"> 32768</span><span style="color:#79B8FF;"> --gpus</span><span style="color:#79B8FF;"> 0</span><span style="color:#79B8FF;"> --use_modelscope</span><span style="color:#79B8FF;"> --gpu_memory_utilization</span><span style="color:#79B8FF;"> 0.9</span><span style="color:#79B8FF;"> --outputs_dir</span><span style="color:#9ECBFF;"> outputs/vllm</span></span></code></pre></div><p>Parameter Explanation:</p><pre><code>`--model_id_or_path`: Model ID or local path, optional values refer to the `Model Resources` section  \n`--context_length`: Input length in tokens; optional values are 1, 6144, 14336, 30720, 63488, 129024; refer to the `Qwen2.5 Model Efficiency Evaluation Report` for specifics  \n`--generate_length`: Number of tokens to generate; default is 2048\n`--max_model_len`: Maximum model length in tokens; default is 32768  \n`--gpus`: Equivalent to the environment variable CUDA_VISIBLE_DEVICES, e.g., `0,1,2,3`, `4,5`   \n`--use_modelscope`: If set, uses ModelScope to load the model; otherwise, uses HuggingFace  \n`--gpu_memory_utilization`: GPU memory utilization, range (0, 1]; default is 0.9  \n`--outputs_dir`: Output directory, default is `outputs/vllm`  \n`--enforce_eager`: Whether to enforce eager mode; default is False  \n</code></pre><h4 id="test-results-1" tabindex="-1">Test Results <a class="header-anchor" href="#test-results-1" aria-label="Permalink to &quot;Test Results&quot;">â€‹</a></h4><p>Test results can be found in the <code>outputs</code> directory, which by default includes two folders for <code>transformers</code> and <code>vllm</code>, storing test results for HuggingFace transformers and vLLM respectively.</p><h2 id="notes" tabindex="-1">Notes <a class="header-anchor" href="#notes" aria-label="Permalink to &quot;Notes&quot;">â€‹</a></h2><ol><li>Conduct multiple tests and take the average, with a typical value of 3 tests.</li><li>Ensure the GPU is idle before testing to avoid interference from other tasks.</li></ol>',45)]))}const h=s(o,[["render",t]]);export{u as __pageData,h as default};
