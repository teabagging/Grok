import{_ as t,c as n,b as a,o}from"./chunks/framework.B1z0IdBH.js";const p=JSON.parse('{"title":"Key Concepts","description":"","frontmatter":{},"headers":[{"level":2,"title":"Qwen","slug":"qwen","link":"#qwen","children":[]},{"level":2,"title":"Causal Language Models","slug":"causal-language-models","link":"#causal-language-models","children":[{"level":3,"title":"Pre-training & Base models","slug":"pre-training-base-models","link":"#pre-training-base-models","children":[]},{"level":3,"title":"Post-training & Instruction-tuned models","slug":"post-training-instruction-tuned-models","link":"#post-training-instruction-tuned-models","children":[]}]},{"level":2,"title":"Tokens & Tokenization","slug":"tokens-tokenization","link":"#tokens-tokenization","children":[{"level":3,"title":"Byte-level Byte Pair Encoding","slug":"byte-level-byte-pair-encoding","link":"#byte-level-byte-pair-encoding","children":[]},{"level":3,"title":"Control Tokens & Chat Template","slug":"control-tokens-chat-template","link":"#control-tokens-chat-template","children":[]}]},{"level":2,"title":"Length Limit","slug":"length-limit","link":"#length-limit","children":[]}],"relativePath":"guide/getting_started/concepts.md","filePath":"guide/getting_started/concepts.md"}'),s={name:"guide/getting_started/concepts.md"};function i(l,e,r,d,u,c){return o(),n("div",null,e[0]||(e[0]=[a(`<h1 id="key-concepts" tabindex="-1">Key Concepts <a class="header-anchor" href="#key-concepts" aria-label="Permalink to &quot;Key Concepts&quot;">​</a></h1><h2 id="qwen" tabindex="-1">Qwen <a class="header-anchor" href="#qwen" aria-label="Permalink to &quot;Qwen&quot;">​</a></h2><p>Qwen (Chinese: 通义千问; pinyin: <em>Tongyi Qianwen</em>) is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc. Both language models and multimodal models are pre-trained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences.</p><p>There is the proprietary version and the open-weight version.</p><p>The proprietary versions include</p><ul><li>Qwen: the language models <ul><li>Qwen Max</li><li>Qwen Plus</li><li>Qwen Turbo</li></ul></li><li>Qwen-VL: the vision-language models <ul><li>Qwen-VL Max</li><li>Qwen-VL Plus</li><li>Qwen-VL OCR</li></ul></li><li>Qwen-Audio: the audio-language models <ul><li>Qwen-Audio Turbo</li><li>Qwen-Audio ASR</li></ul></li></ul><p>You can learn more about them at Alibaba Cloud Model Studio (<a href="https://help.aliyun.com/zh/model-studio/getting-started/models#9f8890ce29g5u" target="_blank" rel="noreferrer">China Site</a> [zh], <a href="https://www.alibabacloud.com/en/product/modelstudio" target="_blank" rel="noreferrer">International Site</a>).</p><p>The spectrum for the open-weight models spans over</p><ul><li>Qwen: the language models <ul><li><a href="https://github.com/QwenLM/Qwen" target="_blank" rel="noreferrer">Qwen</a>: 1.8B, 7B, 14B, and 72B models</li><li><a href="https://github.com/QwenLM/Qwen1.5/tree/v1.5" target="_blank" rel="noreferrer">Qwen1.5</a>: 0.5B, 1.8B, 4B, 14BA2.7B, 7B, 14B, 32B, 72B, and 110B models</li><li><a href="https://github.com/QwenLM/Qwen2/tree/v2.0" target="_blank" rel="noreferrer">Qwen2</a>: 0.5B, 1.5B, 7B, 57A14B, and 72B models</li><li><a href="https://github.com/QwenLM/Qwen2.5/" target="_blank" rel="noreferrer">Qwen2.5</a>: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B models</li></ul></li><li>Qwen-VL: the vision-language models <ul><li><a href="https://github.com/QwenLM/Qwen-VL" target="_blank" rel="noreferrer">Qwen-VL</a>: 7B-based models</li><li><a href="https://github.com/QwenLM/Qwen2-VL" target="_blank" rel="noreferrer">Qwen2-VL</a>: 2B, 7B, and 72B-based models</li></ul></li><li>Qwen-Audio: the audio-language models <ul><li><a href="https://github.com/QwenLM/Qwen-Audio" target="_blank" rel="noreferrer">Qwen-Audio</a>: 7B-based model</li><li><a href="https://github.com/QwenLM/Qwen2-Audio" target="_blank" rel="noreferrer">Qwen2-Audio</a>: 7B-based models</li></ul></li><li>Q*Q: the reasoning models <ul><li><a href="https://github.com/QwenLM/Qwen2.5/" target="_blank" rel="noreferrer">QwQ-Preview</a>: 32B LLM</li><li><a href="https://github.com/QwenLM/Qwen2-VL" target="_blank" rel="noreferrer">QVQ-Preview</a>: 72B VLM</li></ul></li><li>CodeQwen/Qwen-Coder: the language models for coding <ul><li><a href="https://github.com/QwenLM/CodeQwen1.5" target="_blank" rel="noreferrer">CodeQwen1.5</a>: 7B models</li><li><a href="https://github.com/QwenLM/Qwen2.5-Coder" target="_blank" rel="noreferrer">Qwen2.5-Coder</a>: 0.5B, 1.5B, 3B, 7B, 14B, and 32B models</li></ul></li><li>Qwen-Math: the language models for mathematics <ul><li><a href="https://github.com/QwenLM/Qwen2-Math" target="_blank" rel="noreferrer">Qwen2-Math</a>: 1.5B, 7B, and 72B models</li><li><a href="https://github.com/QwenLM/Qwen2.5-Math" target="_blank" rel="noreferrer">Qwen2.5-Math</a>: 1.5B, 7B, and 72B models</li></ul></li><li>Qwen-Math-RM: the reward models for mathematics <ul><li><a href="https://github.com/QwenLM/Qwen2-Math" target="_blank" rel="noreferrer">Qwen2-Math-RM</a>: 72B models</li><li><a href="https://github.com/QwenLM/Qwen2.5-Math" target="_blank" rel="noreferrer">Qwen2.5-Math-RM</a>: 72B models</li><li><a href="https://github.com/QwenLM/Qwen2.5-Math" target="_blank" rel="noreferrer">Qwen2.5-Math-PRM</a>: 7B and 72B models</li></ul></li></ul><p><strong>In this document, our focus is Qwen, the language models.</strong></p><h2 id="causal-language-models" tabindex="-1">Causal Language Models <a class="header-anchor" href="#causal-language-models" aria-label="Permalink to &quot;Causal Language Models&quot;">​</a></h2><p>Causal language models, also known as autoregressive language models or decoder-only language models, are a type of machine learning model designed to predict the next token in a sequence based on the preceding tokens. In other words, they generate text one token at a time, using the previously generated tokens as context. The &quot;causal&quot; aspect refers to the fact that the model only considers the past context (the already generated tokens) when predicting the next token, not any future tokens.</p><p>Causal language models are widely used for various natural language processing tasks involving text completion and generation. They have been particularly successful in generating coherent and contextually relevant text, making them a cornerstone of modern natural language understanding and generation systems.</p><p><strong>Takeaway: Qwen models are causal language models suitable for text completion.</strong></p><p>:::{dropdown} Learn more about language models</p><p>They are three main kinds of models that are commonly referred to as language models in deep learning:</p><ul><li><p>Sequence-to-sequence models: T5 and the likes</p><p>Sequence-to-sequence models use both an encoder to capture the entire input sequence and a decoder to generate an output sequence. They are widely used for tasks like machine translation, text summarization, etc.</p></li><li><p>Bidirectional models or encoder-only models: BERT and the likes</p><p>Bidirectional models can access both past and future context in a sequence during training. They cannot generate sequential outputs in real-time due to the need for future context. They are widely used as embedding models and subsequently used for text classification.</p></li><li><p>Casual language models or decoder-only models: GPT and the likes</p><p>Causal language models operate unidirectionally in a strictly forward direction, predicting each subsequent word based only on the previous words in the sequence. This unidirectional nature ensures that the model&#39;s predictions do not rely on future context, making them suitable for tasks like text completion and generation. :::</p></li></ul><h3 id="pre-training-base-models" tabindex="-1">Pre-training &amp; Base models <a class="header-anchor" href="#pre-training-base-models" aria-label="Permalink to &quot;Pre-training &amp; Base models&quot;">​</a></h3><p>Base language models are foundational models trained on extensive corpora of text to predict the next word in a sequence. Their main goal is to capture the statistical patterns and structures of language, enabling them to generate coherent and contextually relevant text. These models are versatile and can be adapted to various natural language processing tasks through fine-tuning. While adept at producing fluent text, they may require in-context learning or additional training to follow specific instructions or perform complex reasoning tasks effectively. For Qwen models, the base models are those without &quot;-Instruct&quot; indicators, such as Qwen2.5-7B and Qwen2.5-72B.</p><p><strong>Takeaway: Use base models for in-context learning, downstream fine-tuning, etc.</strong></p><h3 id="post-training-instruction-tuned-models" tabindex="-1">Post-training &amp; Instruction-tuned models <a class="header-anchor" href="#post-training-instruction-tuned-models" aria-label="Permalink to &quot;Post-training &amp; Instruction-tuned models&quot;">​</a></h3><p>Instruction-tuned language models are specialized models designed to understand and execute specific instructions in conversational styles. These models are fine-tuned to interpret user commands accurately and can perform tasks such as summarization, translation, and question answering with improved accuracy and consistency. Unlike base models, which are trained on large corpora of text, instruction-tuned models undergo additional training using datasets that contain examples of instructions and their desired outcomes, often in multiple turns. This kind of training makes them ideal for applications requiring targeted functionalities while maintaining the ability to generate fluent and coherent text. For Qwen models, the instruction-tuned models are those with the &quot;-Instruct&quot; suffix, such as Qwen2.5-7B-Instruct and Qwen2.5-72B-Instruct. [^instruct-chat]</p><p><strong>Takeaway: Use instruction-tuned models for conducting tasks in conversations, downstream fine-tuning, etc.</strong></p><p>[^instruct-chat]: Previously, they are known as the chat models and with the &quot;-Chat&quot; suffix. Starting from Qwen2, the name is changed to follow the common practice. For Qwen, &quot;-Instruct&quot; and &quot;-Chat&quot; should be regarded as synonymous.</p><h2 id="tokens-tokenization" tabindex="-1">Tokens &amp; Tokenization <a class="header-anchor" href="#tokens-tokenization" aria-label="Permalink to &quot;Tokens &amp; Tokenization&quot;">​</a></h2><p>Tokens represent the fundamental units that models process and generate. They can represent texts in human languages (regular tokens) or represent specific functionality like keywords in programming languages (control tokens [^special]). Typically, a tokenizer is used to split text into regular tokens, which can be words, subwords, or characters depending on the specific tokenization scheme employed, and furnish the token sequence with control tokens as needed. The vocabulary size, or the total number of unique tokens a model recognizes, significantly impacts its performance and versatility. Larger language models often use sophisticated tokenization methods to handle the vast diversity of human language while keeping the vocabulary size manageable. Qwen use a relatively large vocabulary of 151,646 tokens in total.</p><p>[^special]: Control tokens can be called special tokens. However, the meaning of special tokens need to be interpreted based on the contexts: special tokens may contain extra regular tokens.</p><p><strong>Takeaway: Tokenization method and vocabulary size is important.</strong></p><h3 id="byte-level-byte-pair-encoding" tabindex="-1">Byte-level Byte Pair Encoding <a class="header-anchor" href="#byte-level-byte-pair-encoding" aria-label="Permalink to &quot;Byte-level Byte Pair Encoding&quot;">​</a></h3><p>Qwen adopts a subword tokenization method called Byte Pair Encoding (BPE), which attempts to learn the composition of tokens that can represent the text with the fewest tokens. For example, the string &quot; tokenization&quot; is decomposed as &quot; token&quot; and &quot;ization&quot; (note that the space is part of the token). Especially, the tokenization of Qwen ensures that there is no unknown words and all texts can be transformed to token sequences.</p><p>There are 151,643 tokens as a result of BPE in the vocabulary of Qwen, which is a large vocabulary efficient for diverse languages. As a rule of thumb, 1 token is 3~4 characters for English texts and 1.5~1.8 characters for Chinese texts.</p><p><strong>Takeaway: Qwen processes texts in subwords and there are no unknown words.</strong></p><p>:::{dropdown} Learn more about tokenization in Qwen Qwen uses byte-level BPE (BBPE) on UTF-8 encoded texts. It starts by treating each byte as a token and then iteratively merges the most frequent pairs of tokens occurring the texts into larger tokens until the desired vocabulary size is met.</p><p>In byte-level BPE, minimum 256 tokens are needed to tokenize every piece of text and avoid the out of vocabulary (OOV) problem. In comparison, character-level BPE needs every Unicode character in its vocabulary to avoid OOV and the Unicode Standard contains 154,998 characters as of Unicode Version 16.0.</p><p>One limitation to keep in mind for byte-level BPE is that the individual tokens in the vocabulary may not be seemingly semantically meaningful or even valid UTF-8 byte sequences, and in certain aspects, they should be viewed as a text compression scheme. :::</p><h3 id="control-tokens-chat-template" tabindex="-1">Control Tokens &amp; Chat Template <a class="header-anchor" href="#control-tokens-chat-template" aria-label="Permalink to &quot;Control Tokens &amp; Chat Template&quot;">​</a></h3><p>Control tokens and chat templates both serve as mechanisms to guide the model&#39;s behavior and outputs.</p><p>Control tokens are special tokens inserted into the sequence that signifies meta information. For example, in pre-training, multiple documents may be packed into a single sequence. For Qwen, the control token &quot;&lt;|endoftext|&gt;&quot; is inserted after each document to signify that the document has ended and a new document will proceed.</p><p>Chat templates provide a structured format for conversational interactions, where predefined placeholders or prompts are used to elicit responses from the model that adhere to a desired dialogue flow or context. Different models may use different kinds of chat template to format the conversations. It is crucial to use the designated one to ensure the precise control over the LLM&#39;s generation process.</p><p>Qwen uses the following format (ChatML[^chatml]), making use of control tokens to format each turn in the conversations</p><div class="language-text"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>&lt;|im_start|&gt;{{role}}</span></span>
<span class="line"><span>{{content}}&lt;|im_end|&gt;</span></span></code></pre></div><p>The user input take the role of <code>user</code> and the model generation takes the role of <code>assistant</code>. Qwen also supports the meta message that instruct the model to perform specific actions or generate text with certain characteristics, such as altering tone, style, or content, which takes the role of <code>system</code> and the content defaults to &quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</p><p>The following is a full example:</p><div class="language-text"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span>&lt;|im_start|&gt;system</span></span>
<span class="line"><span>You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&lt;|im_end|&gt;</span></span>
<span class="line"><span>&lt;|im_start|&gt;user</span></span>
<span class="line"><span>hello&lt;|im_end|&gt;</span></span>
<span class="line"><span>&lt;|im_start|&gt;assistant</span></span>
<span class="line"><span>Hello! How can I assist you today? Let me know if you have any questions or need help with something. I&#39;m here to help!&lt;|im_end|&gt;</span></span>
<span class="line"><span>&lt;|im_start|&gt;user</span></span>
<span class="line"><span>Explain large language models like I&#39;m 5.&lt;|im_end|&gt;</span></span>
<span class="line"><span>&lt;|im_start|&gt;assistant</span></span>
<span class="line"><span>Sure, I&#39;d be happy to explain large language models in a simple way!</span></span>
<span class="line"><span>Imagine that you have a friend who is really good at playing with words. This friend has read so many books and talked to so many people that they know how to use words in all sorts of different ways. They can tell stories, answer questions, and even make up new words if they need to.</span></span>
<span class="line"><span>Now imagine that this friend is actually a computer program, called a &quot;large language model&quot;. It&#39;s been trained on lots and lots of text, like books, articles, and conversations, so it knows how to use words just like your word-loving friend does. When you ask the model a question or give it a task, it uses all the knowledge it&#39;s learned to come up with a response that makes sense.</span></span>
<span class="line"><span>Just like your friend might sometimes make mistakes or say things in a funny way, the large language model isn&#39;t perfect either. But it&#39;s still really impressive, because it can understand and generate human language in a way that was once thought impossible for machines!&lt;|im_end|&gt;&lt;|endoftext|&gt;</span></span></code></pre></div><p>Starting from Qwen2.5, the Qwen model family including multimodal and specialized models will use a unified vocabulary, which contains control tokens from all subfamilies. There are 22 control tokens in the vocabulary of Qwen2.5, making the vocabulary size totaling 151,665:</p><ul><li>1 general: <code>&lt;|endoftext|&gt;</code></li><li>2 for chat: <code>&lt;|im_start|&gt;</code> and <code>&lt;|im_end|&gt;</code></li><li>2 for tool use: <code>&lt;tool_call&gt;</code> and <code>&lt;/tool_call&gt;</code></li><li>11 for vision</li><li>6 for coding</li></ul><p><strong>Takeaway: Qwen uses ChatML with control tokens for chat template.</strong></p><p>[^chatml]: For historical reference only, ChatML is first described by the OpenAI Python SDK. The last available version is <a href="https://github.com/openai/openai-python/blob/v0.28.1/chatml.md" target="_blank" rel="noreferrer">this</a>. Please also be aware that that document lists use cases intended for OpenAI models. For Qwen2.5 models, please only use as in our guide.</p><h2 id="length-limit" tabindex="-1">Length Limit <a class="header-anchor" href="#length-limit" aria-label="Permalink to &quot;Length Limit&quot;">​</a></h2><p>As Qwen models are causal language models, in theory there is only one length limit of the entire sequence. However, since there is often packing in training and each sequence may contain multiple individual pieces of texts. <strong>How long the model can generate or complete ultimately depends on the use case and in that case how long each document (for pre-training) or each turn (for post-training) is in training.</strong></p><p>For Qwen2.5, the packed sequence length in training is 32,768 tokens.[^yarn] The maximum document length in pre-training is this length. The maximum message length for user and assistant is different in post-training. In general, the assistant message could be up to 8192 tokens.</p><p>[^yarn]: The sequence length can be extended to 131,072 tokens for Qwen2.5-7B, Qwen2.5-14B, Qwen2.5-32B, and Qwen2.5-72B models with YaRN. Please refer to the model card on how to enable YaRN in vLLM.</p><p><strong>Takeaway: Qwen2.5 models can process texts of 32K or 128K tokens and up to 8K tokens can be assistant output.</strong></p>`,53)]))}const g=t(s,[["render",i]]);export{p as __pageData,g as default};
