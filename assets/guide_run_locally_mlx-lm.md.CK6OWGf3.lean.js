import{_ as a,c as n,b as e,o as l}from"./chunks/framework.B1z0IdBH.js";const d=JSON.parse('{"title":"MLX-LM","description":"","frontmatter":{},"headers":[{"level":2,"title":"Prerequisites","slug":"prerequisites","link":"#prerequisites","children":[]},{"level":2,"title":"Running with Qwen MLX Files","slug":"running-with-qwen-mlx-files","link":"#running-with-qwen-mlx-files","children":[]},{"level":2,"title":"Make Your MLX files","slug":"make-your-mlx-files","link":"#make-your-mlx-files","children":[]}],"relativePath":"guide/run_locally/mlx-lm.md","filePath":"guide/run_locally/mlx-lm.md"}'),o={name:"guide/run_locally/mlx-lm.md"};function p(t,s,r,c,i,y){return l(),n("div",null,s[0]||(s[0]=[e(`<h1 id="mlx-lm" tabindex="-1">MLX-LM <a class="header-anchor" href="#mlx-lm" aria-label="Permalink to &quot;MLX-LM&quot;">​</a></h1><p><a href="https://github.com/ml-explore/mlx-examples/tree/main/llms" target="_blank" rel="noreferrer">mlx-lm</a> helps you run LLMs locally on Apple Silicon. It is available at MacOS. It has already supported Qwen models and this time, we have also provided checkpoints that you can directly use with it.</p><h2 id="prerequisites" tabindex="-1">Prerequisites <a class="header-anchor" href="#prerequisites" aria-label="Permalink to &quot;Prerequisites&quot;">​</a></h2><p>The easiest way to get started is to install the <code>mlx-lm</code> package:</p><ul><li><p>with <code>pip</code>:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">pip</span><span style="color:#9ECBFF;"> install</span><span style="color:#9ECBFF;"> mlx-lm</span></span></code></pre></div></li><li><p>with <code>conda</code>:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">conda</span><span style="color:#9ECBFF;"> install</span><span style="color:#79B8FF;"> -c</span><span style="color:#9ECBFF;"> conda-forge</span><span style="color:#9ECBFF;"> mlx-lm</span></span></code></pre></div></li></ul><h2 id="running-with-qwen-mlx-files" tabindex="-1">Running with Qwen MLX Files <a class="header-anchor" href="#running-with-qwen-mlx-files" aria-label="Permalink to &quot;Running with Qwen MLX Files&quot;">​</a></h2><p>We provide model checkpoints with <code>mlx-lm</code> in our Hugging Face organization, and to search for what you need you can search the repo names with <code>-MLX</code>.</p><p>Here provides a code snippet with <code>apply_chat_template</code> to show you how to load the tokenizer and model and how to generate contents.</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> mlx_lm </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> load, generate</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">model, tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> load(</span><span style="color:#9ECBFF;">&#39;Qwen/Qwen2.5-7B-Instruct-MLX&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">tokenizer_config</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">{</span><span style="color:#9ECBFF;">&quot;eos_token&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;&lt;|im_end|&gt;&quot;</span><span style="color:#E1E4E8;">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">prompt </span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;"> &quot;Give me a short introduction to large language model.&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">messages </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;system&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span><span style="color:#E1E4E8;">},</span></span>
<span class="line"><span style="color:#E1E4E8;">    {</span><span style="color:#9ECBFF;">&quot;role&quot;</span><span style="color:#E1E4E8;">: </span><span style="color:#9ECBFF;">&quot;user&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;content&quot;</span><span style="color:#E1E4E8;">: prompt}</span></span>
<span class="line"><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="color:#E1E4E8;">    messages,</span></span>
<span class="line"><span style="color:#FFAB70;">    tokenize</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#FFAB70;">    add_generation_prompt</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span></span>
<span class="line"><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">response </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> generate(model, tokenizer, </span><span style="color:#FFAB70;">prompt</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">text, </span><span style="color:#FFAB70;">verbose</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">top_p</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.8</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">temp</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.7</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">repetition_penalty</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1.05</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">max_tokens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">)</span></span></code></pre></div><h2 id="make-your-mlx-files" tabindex="-1">Make Your MLX files <a class="header-anchor" href="#make-your-mlx-files" aria-label="Permalink to &quot;Make Your MLX files&quot;">​</a></h2><p>You can make mlx files with just one command:</p><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#B392F0;">mlx_lm.convert</span><span style="color:#79B8FF;"> --hf-path</span><span style="color:#9ECBFF;"> Qwen/Qwen2.5-7B-Instruct</span><span style="color:#79B8FF;"> --mlx-path</span><span style="color:#9ECBFF;"> mlx/Qwen2.5-7B-Instruct/</span><span style="color:#79B8FF;"> -q</span></span></code></pre></div><p>where</p><ul><li><code>--hf-path</code>: the model name on Hugging Face Hub or the local path</li><li><code>--mlx-path</code>: the path for output files</li><li><code>-q</code>: enable quantization</li></ul>`,14)]))}const u=a(o,[["render",p]]);export{d as __pageData,u as default};
